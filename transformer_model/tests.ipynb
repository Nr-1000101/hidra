{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1028aafe-c0b3-4c0d-8c5b-75bb56d9678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_load_vocab (__main__.TestVocabLoader.test_load_vocab)\n",
      "Test vocabulary loading returns correct structures. ... ok\n",
      "test_fit_and_transform (__main__.TestLabelEncoder.test_fit_and_transform)\n",
      "Test fitting and transforming labels. ... ok\n",
      "test_multiple_properties (__main__.TestLabelEncoder.test_multiple_properties)\n",
      "Test encoding multiple different properties. ... ok\n",
      "test_num_classes (__main__.TestLabelEncoder.test_num_classes)\n",
      "Test getting number of classes. ... ok\n",
      "test_unknown_label (__main__.TestLabelEncoder.test_unknown_label)\n",
      "Test handling unknown labels (should default to 0). ... ok\n",
      "test_selfies_tokenizer_encode (__main__.TestTokenizers.test_selfies_tokenizer_encode)\n",
      "Test SELFIES tokenizer encoding. ... ok\n",
      "test_smiles_regex_tokenization (__main__.TestTokenizers.test_smiles_regex_tokenization)\n",
      "Test SMILES regex tokenizer handles multi-char atoms. ... ok\n",
      "test_smiles_tokenizer_decode (__main__.TestTokenizers.test_smiles_tokenizer_decode)\n",
      "Test SMILES tokenizer decoding. ... ok\n",
      "test_smiles_tokenizer_encode (__main__.TestTokenizers.test_smiles_tokenizer_encode)\n",
      "Test SMILES tokenizer encoding. ... ok\n",
      "test_smiles_tokenizer_roundtrip (__main__.TestTokenizers.test_smiles_tokenizer_roundtrip)\n",
      "Test SMILES encode-decode roundtrip. ... ok\n",
      "test_smiles_tokenizer_vocab_size (__main__.TestTokenizers.test_smiles_tokenizer_vocab_size)\n",
      "Test SMILES tokenizer vocab size. ... ok\n",
      "test_unknown_tokens (__main__.TestTokenizers.test_unknown_tokens)\n",
      "Test handling of unknown tokens. ... ok\n",
      "test_collate_fn (__main__.TestDataset.test_collate_fn)\n",
      "Test collate function properly batches data. ... ok\n",
      "test_dataset_getitem (__main__.TestDataset.test_dataset_getitem)\n",
      "Test dataset __getitem__ returns correct format. ... ok\n",
      "test_dataset_length (__main__.TestDataset.test_dataset_length)\n",
      "Test dataset returns correct length. ... ok\n",
      "test_dataset_padding (__main__.TestDataset.test_dataset_padding)\n",
      "Test sequences are properly padded. ... ok\n",
      "test_cross_attention (__main__.TestModelComponents.test_cross_attention)\n",
      "Test cross-attention module. ... ok\n",
      "test_cross_attention_with_mask (__main__.TestModelComponents.test_cross_attention_with_mask)\n",
      "Test cross-attention with padding mask. ... ok\n",
      "test_positional_encoding (__main__.TestModelComponents.test_positional_encoding)\n",
      "Test learned positional encoding. ... ok\n",
      "test_positional_encoding_exceeds_max_len (__main__.TestModelComponents.test_positional_encoding_exceeds_max_len)\n",
      "Test positional encoding raises error for sequences too long. ... ok\n",
      "test_property_block_classification (__main__.TestModelComponents.test_property_block_classification)\n",
      "Test property block for classification task. ... ok\n",
      "test_property_block_regression (__main__.TestModelComponents.test_property_block_regression)\n",
      "Test property block for regression task. ... ok\n",
      "test_property_block_with_previous_memory (__main__.TestModelComponents.test_property_block_with_previous_memory)\n",
      "Test property block with cross-attention to previous property. ... ok\n",
      "test_model_forward (__main__.TestHierarchicalTransformer.test_model_forward)\n",
      "Test forward pass produces correct output shapes. ... ok\n",
      "test_model_gradient_flow (__main__.TestHierarchicalTransformer.test_model_gradient_flow)\n",
      "Test gradients flow through entire model. ... ok\n",
      "test_model_initialization (__main__.TestHierarchicalTransformer.test_model_initialization)\n",
      "Test model initializes without errors. ... ok\n",
      "test_model_parameter_count (__main__.TestHierarchicalTransformer.test_model_parameter_count)\n",
      "Test model has reasonable number of parameters. ... ok\n",
      "test_model_with_padding (__main__.TestHierarchicalTransformer.test_model_with_padding)\n",
      "Test model handles padded sequences correctly. ... ok\n",
      "test_evaluate (__main__.TestTrainingFunctions.test_evaluate)\n",
      "Test evaluation runs without errors. ... C:\\Users\\turih\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "ok\n",
      "test_evaluation_no_gradient (__main__.TestTrainingFunctions.test_evaluation_no_gradient)\n",
      "Test evaluation doesn't compute gradients. ... ok\n",
      "test_train_one_epoch (__main__.TestTrainingFunctions.test_train_one_epoch)\n",
      "Test training for one epoch runs without errors. ... ok\n",
      "test_training_updates_parameters (__main__.TestTrainingFunctions.test_training_updates_parameters)\n",
      "Test that training actually updates model parameters. ... ok\n",
      "test_full_pipeline (__main__.TestIntegration.test_full_pipeline)\n",
      "Test complete pipeline: data loading → model → training → evaluation. ... ok\n",
      "test_model_save_load (__main__.TestIntegration.test_model_save_load)\n",
      "Test saving and loading model checkpoint. ... ok\n",
      "test_overfitting_simple_data (__main__.TestIntegration.test_overfitting_simple_data)\n",
      "Test model can overfit to small dataset (sanity check). ... ok\n",
      "test_empty_sequence (__main__.TestEdgeCases.test_empty_sequence)\n",
      "Test handling of empty sequences. ... ok\n",
      "test_invalid_property_config (__main__.TestEdgeCases.test_invalid_property_config)\n",
      "Test model rejects invalid property configurations. ... ok\n",
      "test_mismatched_batch_sizes (__main__.TestEdgeCases.test_mismatched_batch_sizes)\n",
      "Test error handling for mismatched batch sizes. ... ok\n",
      "test_very_long_sequence (__main__.TestEdgeCases.test_very_long_sequence)\n",
      "Test truncation of very long sequences. ... ok\n",
      "test_assign_binned_labels_above_threshold (__main__.TestDatasetMethods.test_assign_binned_labels_above_threshold)\n",
      "Test assign_binned_labels for values at or above threshold. ... ok\n",
      "test_assign_binned_labels_below_threshold (__main__.TestDatasetMethods.test_assign_binned_labels_below_threshold)\n",
      "Test assign_binned_labels for values below threshold. ... ok\n",
      "test_assign_other_label (__main__.TestDatasetMethods.test_assign_other_label)\n",
      "Test assign_other_label method. ... ok\n",
      "test_dataset_multi_file_handling (__main__.TestDatasetMethods.test_dataset_multi_file_handling)\n",
      "Test dataset with multiple HDF5 files. ... ok\n",
      "test_dataset_with_max_molecules (__main__.TestDatasetMethods.test_dataset_with_max_molecules)\n",
      "Test dataset with max_molecules parameter. ... ok\n",
      "test_smiles_bonds (__main__.TestTokenizerEdgeCases.test_smiles_bonds)\n",
      "Test SMILES tokenization of different bond types. ... ok\n",
      "test_smiles_bracketed_atoms (__main__.TestTokenizerEdgeCases.test_smiles_bracketed_atoms)\n",
      "Test SMILES tokenization of bracketed atoms. ... ok\n",
      "test_smiles_multi_char_atoms (__main__.TestTokenizerEdgeCases.test_smiles_multi_char_atoms)\n",
      "Test SMILES tokenization of multi-character atoms. ... ok\n",
      "test_smiles_stereochemistry (__main__.TestTokenizerEdgeCases.test_smiles_stereochemistry)\n",
      "Test SMILES tokenization of stereochemistry markers. ... ok\n",
      "test_smiles_two_digit_ring_closure (__main__.TestTokenizerEdgeCases.test_smiles_two_digit_ring_closure)\n",
      "Test SMILES tokenization of %NN ring closures. ... ok\n",
      "test_all_classification_properties (__main__.TestModelArchitectureVariations.test_all_classification_properties)\n",
      "Test model with all classification properties. ... ok\n",
      "test_all_regression_properties (__main__.TestModelArchitectureVariations.test_all_regression_properties)\n",
      "Test model with all regression properties. ... ok\n",
      "test_different_dim_feedforward (__main__.TestModelArchitectureVariations.test_different_dim_feedforward)\n",
      "Test model with different feedforward dimensions. ... ok\n",
      "test_different_n_initial_blocks (__main__.TestModelArchitectureVariations.test_different_n_initial_blocks)\n",
      "Test model with varying numbers of initial encoder blocks. ... ok\n",
      "test_different_num_heads (__main__.TestModelArchitectureVariations.test_different_num_heads)\n",
      "Test model with different numbers of attention heads. ... ok\n",
      "test_varying_n_blocks_per_property (__main__.TestModelArchitectureVariations.test_varying_n_blocks_per_property)\n",
      "Test model with different n_blocks for each property. ... ok\n",
      "test_classification_without_num_classes (__main__.TestErrorHandling.test_classification_without_num_classes)\n",
      "Test error for classification task without num_classes. ... ok\n",
      "test_invalid_task_type (__main__.TestErrorHandling.test_invalid_task_type)\n",
      "Test error for invalid task type in PropertyBlock. ... ok\n",
      "test_label_encoder_empty_property (__main__.TestErrorHandling.test_label_encoder_empty_property)\n",
      "Test label encoder with property that hasn't been fit. ... ok\n",
      "test_missing_vocab_file (__main__.TestErrorHandling.test_missing_vocab_file)\n",
      "Test error when vocab file doesn't exist. ... ok\n",
      "test_model_with_mismatched_attention_heads (__main__.TestErrorHandling.test_model_with_mismatched_attention_heads)\n",
      "Test error when d_model not divisible by nhead. ... ok\n",
      "test_dropout_in_train_vs_eval (__main__.TestDeterminism.test_dropout_in_train_vs_eval)\n",
      "Test dropout behaves differently in train vs eval mode. ... ok\n",
      "test_model_forward_determinism (__main__.TestDeterminism.test_model_forward_determinism)\n",
      "Test model produces same outputs with same inputs in eval mode. ... ok\n",
      "test_positional_encoding_determinism (__main__.TestDeterminism.test_positional_encoding_determinism)\n",
      "Test positional encoding produces same results with same seed. ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 63 tests in 6.406s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLETE TEST SUMMARY (ORIGINAL + EXTENDED)\n",
      "======================================================================\n",
      "Tests run: 63\n",
      "Successes: 63\n",
      "Failures: 0\n",
      "Errors: 0\n",
      "\n",
      "✓ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive test suite for transformer.py\n",
    "\n",
    "Tests all components individually and integration:\n",
    "- Tokenizers (SMILES/SELFIES)\n",
    "- LabelEncoder\n",
    "- Dataset loading\n",
    "- Model components (positional encoding, cross-attention, property blocks)\n",
    "- Full hierarchical transformer\n",
    "- Training/evaluation functions\n",
    "\n",
    "Usage:\n",
    "    python test_transformer.py\n",
    "    \n",
    "Or run specific test:\n",
    "    python test_transformer.py TestTokenizers.test_smiles_tokenizer\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import all components from transformer\n",
    "from transformer import (\n",
    "    load_vocab,\n",
    "    LabelEncoder,\n",
    "    SelfiesTokenizer,\n",
    "    SmilesTokenizer,\n",
    "    H5SequenceDataset,\n",
    "    collate_fn,\n",
    "    LearnedPositionalEncoding,\n",
    "    CrossAttention,\n",
    "    PropertyBlock,\n",
    "    HierarchicalTransformer,\n",
    "    train_one_epoch,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "\n",
    "class TestVocabLoader(unittest.TestCase):\n",
    "    \"\"\"Test vocabulary loading functionality.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Create temporary vocab file.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        self.vocab_file = os.path.join(self.temp_dir, \"test_vocab.json\")\n",
    "        \n",
    "        vocab_data = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\"],\n",
    "            \"token_to_id\": {\n",
    "                \"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3,\n",
    "                \"C\": 4, \"N\": 5, \"O\": 6\n",
    "            },\n",
    "            \"id_to_token\": {\n",
    "                \"0\": \"<pad>\", \"1\": \"<unk>\", \"2\": \"<bos>\", \"3\": \"<eos>\",\n",
    "                \"4\": \"C\", \"5\": \"N\", \"6\": \"O\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(self.vocab_file, \"w\") as f:\n",
    "            json.dump(vocab_data, f)\n",
    "    \n",
    "    def test_load_vocab(self):\n",
    "        \"\"\"Test vocabulary loading returns correct structures.\"\"\"\n",
    "        tokens, token_to_id, id_to_token = load_vocab(self.vocab_file)\n",
    "        \n",
    "        self.assertEqual(len(tokens), 7)\n",
    "        self.assertIn(\"<pad>\", tokens)\n",
    "        self.assertEqual(token_to_id[\"C\"], 4)\n",
    "        self.assertEqual(id_to_token[4], \"C\")\n",
    "        self.assertIsInstance(id_to_token, dict)\n",
    "        self.assertTrue(all(isinstance(k, int) for k in id_to_token.keys()))\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        if os.path.exists(self.vocab_file):\n",
    "            os.remove(self.vocab_file)\n",
    "        os.rmdir(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestLabelEncoder(unittest.TestCase):\n",
    "    \"\"\"Test LabelEncoder functionality.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Initialize label encoder.\"\"\"\n",
    "        self.encoder = LabelEncoder()\n",
    "    \n",
    "    def test_fit_and_transform(self):\n",
    "        \"\"\"Test fitting and transforming labels.\"\"\"\n",
    "        labels = [\"linear\", \"planar\", \"tetrahedral\", \"linear\", \"planar\"]\n",
    "        self.encoder.fit(\"dimension\", labels)\n",
    "        \n",
    "        # Check encoding\n",
    "        idx = self.encoder.transform(\"dimension\", \"linear\")\n",
    "        self.assertIsInstance(idx, int)\n",
    "        self.assertGreaterEqual(idx, 0)\n",
    "        \n",
    "        # Check inverse\n",
    "        label = self.encoder.inverse_transform(\"dimension\", idx)\n",
    "        self.assertEqual(label, \"linear\")\n",
    "    \n",
    "    def test_num_classes(self):\n",
    "        \"\"\"Test getting number of classes.\"\"\"\n",
    "        labels = [\"A\", \"B\", \"C\", \"A\", \"B\"]\n",
    "        self.encoder.fit(\"test_prop\", labels)\n",
    "        \n",
    "        self.assertEqual(self.encoder.get_num_classes(\"test_prop\"), 3)\n",
    "    \n",
    "    def test_unknown_label(self):\n",
    "        \"\"\"Test handling unknown labels (should default to 0).\"\"\"\n",
    "        self.encoder.fit(\"dimension\", [\"linear\", \"planar\"])\n",
    "        idx = self.encoder.transform(\"dimension\", \"unknown_label\")\n",
    "        self.assertEqual(idx, 0)\n",
    "    \n",
    "    def test_multiple_properties(self):\n",
    "        \"\"\"Test encoding multiple different properties.\"\"\"\n",
    "        self.encoder.fit(\"prop1\", [\"A\", \"B\", \"C\"])\n",
    "        self.encoder.fit(\"prop2\", [\"X\", \"Y\"])\n",
    "        \n",
    "        self.assertEqual(self.encoder.get_num_classes(\"prop1\"), 3)\n",
    "        self.assertEqual(self.encoder.get_num_classes(\"prop2\"), 2)\n",
    "        \n",
    "        # Properties are independent, but both A and X get sorted to index 0\n",
    "        # This test is actually checking wrong behavior - remove the assertion\n",
    "        # Both will be 0 since they're the first alphabetically in their respective groups\n",
    "\n",
    "\n",
    "class TestTokenizers(unittest.TestCase):\n",
    "    \"\"\"Test SMILES and SELFIES tokenizers.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Create temporary vocab files.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # SMILES vocab\n",
    "        smiles_vocab = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"(\", \")\", \"=\", \"1\"],\n",
    "            \"token_to_id\": {\n",
    "                \"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3,\n",
    "                \"C\": 4, \"N\": 5, \"O\": 6, \"(\": 7, \")\": 8, \"=\": 9, \"1\": 10\n",
    "            },\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"(\", \")\", \"=\", \"1\"]\n",
    "            )}\n",
    "        }\n",
    "        \n",
    "        self.smiles_vocab_file = os.path.join(self.temp_dir, \"smiles_vocab.json\")\n",
    "        with open(self.smiles_vocab_file, \"w\") as f:\n",
    "            json.dump(smiles_vocab, f)\n",
    "        \n",
    "        # SELFIES vocab\n",
    "        selfies_vocab = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"[C]\", \"[N]\", \"[O]\", \"[=C]\"],\n",
    "            \"token_to_id\": {\n",
    "                \"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3,\n",
    "                \"[C]\": 4, \"[N]\": 5, \"[O]\": 6, \"[=C]\": 7\n",
    "            },\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"[C]\", \"[N]\", \"[O]\", \"[=C]\"]\n",
    "            )}\n",
    "        }\n",
    "        \n",
    "        self.selfies_vocab_file = os.path.join(self.temp_dir, \"selfies_vocab.json\")\n",
    "        with open(self.selfies_vocab_file, \"w\") as f:\n",
    "            json.dump(selfies_vocab, f)\n",
    "    \n",
    "    def test_smiles_tokenizer_encode(self):\n",
    "        \"\"\"Test SMILES tokenizer encoding.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Test with special tokens\n",
    "        ids = tokenizer.encode(\"C=O\", add_special=True)\n",
    "        self.assertEqual(ids[0], 2)  # BOS\n",
    "        self.assertEqual(ids[-1], 3)  # EOS\n",
    "        self.assertGreater(len(ids), 2)\n",
    "        \n",
    "        # Test without special tokens\n",
    "        ids_no_special = tokenizer.encode(\"C=O\", add_special=False)\n",
    "        self.assertEqual(len(ids_no_special), len(ids) - 2)\n",
    "    \n",
    "    def test_smiles_tokenizer_decode(self):\n",
    "        \"\"\"Test SMILES tokenizer decoding.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        ids = [2, 4, 9, 6, 3]  # <bos> C = O <eos>\n",
    "        decoded = tokenizer.decode(ids)\n",
    "        self.assertEqual(decoded, \"C=O\")\n",
    "    \n",
    "    def test_smiles_tokenizer_roundtrip(self):\n",
    "        \"\"\"Test SMILES encode-decode roundtrip.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        original = \"C=O\"\n",
    "        ids = tokenizer.encode(original, add_special=True)\n",
    "        decoded = tokenizer.decode(ids)\n",
    "        self.assertEqual(decoded, original)\n",
    "    \n",
    "    def test_smiles_tokenizer_vocab_size(self):\n",
    "        \"\"\"Test SMILES tokenizer vocab size.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        self.assertEqual(tokenizer.vocab_size, 11)\n",
    "    \n",
    "    def test_smiles_regex_tokenization(self):\n",
    "        \"\"\"Test SMILES regex tokenizer handles multi-char atoms.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Should tokenize Cl as single token\n",
    "        tokens = tokenizer.tokenize(\"CCl\")\n",
    "        self.assertIn(\"Cl\", tokens)\n",
    "        self.assertEqual(len(tokens), 2)  # C and Cl\n",
    "    \n",
    "    def test_selfies_tokenizer_encode(self):\n",
    "        \"\"\"Test SELFIES tokenizer encoding.\"\"\"\n",
    "        tokenizer = SelfiesTokenizer(self.selfies_vocab_file)\n",
    "        \n",
    "        ids = tokenizer.encode(\"[C][O]\", add_special=True)\n",
    "        self.assertEqual(ids[0], 2)  # BOS\n",
    "        self.assertEqual(ids[-1], 3)  # EOS\n",
    "    \n",
    "    def test_unknown_tokens(self):\n",
    "        \"\"\"Test handling of unknown tokens.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Token not in vocab should map to <unk>\n",
    "        ids = tokenizer.encode(\"CXO\", add_special=False)  # X not in vocab\n",
    "        self.assertIn(1, ids)  # <unk> id\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        for f in [self.smiles_vocab_file, self.selfies_vocab_file]:\n",
    "            if os.path.exists(f):\n",
    "                os.remove(f)\n",
    "        os.rmdir(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestDataset(unittest.TestCase):\n",
    "    \"\"\"Test H5SequenceDataset functionality.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Create temporary HDF5 files and vocab.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create mock molecule file\n",
    "        self.mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        with h5py.File(self.mol_file, \"w\") as f:\n",
    "            smiles_data = [b\"CCO\", b\"CCCO\", b\"C=O\"]\n",
    "            f.create_dataset(\"smiles\", data=np.array(smiles_data, dtype=\"S\"))\n",
    "        \n",
    "        # Create mock feature file\n",
    "        self.feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        with h5py.File(self.feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\", b\"planar\", b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\", b\"Cs\", b\"C2v\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0, 1, 2], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0, 1, 0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0, 1, 0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1, 0.2, 0.15], dtype=float))\n",
    "            \n",
    "            # Create plane_angles structured array\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            angles = np.array([], dtype=dt)\n",
    "            f.create_dataset(\"plane_angles\", data=angles)\n",
    "        \n",
    "        # Create underrepresented data file\n",
    "        self.underrep_file = os.path.join(self.temp_dir, \"underrep.json\")\n",
    "        with open(self.underrep_file, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"point_groups\": [\"rare_group\"],\n",
    "                \"symmetry_planes\": 5,\n",
    "                \"nrings\": 4\n",
    "            }, f)\n",
    "        \n",
    "        # Create vocab\n",
    "        self.vocab_file = os.path.join(self.temp_dir, \"vocab.json\")\n",
    "        vocab_data = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"O\", \"=\"],\n",
    "            \"token_to_id\": {\n",
    "                \"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3,\n",
    "                \"C\": 4, \"O\": 5, \"=\": 6\n",
    "            },\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"O\", \"=\"]\n",
    "            )}\n",
    "        }\n",
    "        with open(self.vocab_file, \"w\") as f:\n",
    "            json.dump(vocab_data, f)\n",
    "        \n",
    "        # Initialize tokenizer and label encoder as instance variables\n",
    "        self.tokenizer = SmilesTokenizer(self.vocab_file)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(\"dimension\", [\"linear\", \"planar\"])\n",
    "        self.label_encoder.fit(\"ring_count\", [\"0\", \"1\", \"4+\"])\n",
    "        self.label_encoder.fit(\"n_symmetry_planes\", [\"0\", \"1\", \"2\", \"5+\"])\n",
    "        self.label_encoder.fit(\"point_group\", [\"C1\", \"Cs\", \"C2v\", \"Other\"])\n",
    "    \n",
    "    def test_dataset_length(self):\n",
    "        \"\"\"Test dataset returns correct length.\"\"\"\n",
    "        dataset = H5SequenceDataset(\n",
    "            [self.mol_file],\n",
    "            [self.feat_file],\n",
    "            self.tokenizer,\n",
    "            self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=32\n",
    "        )\n",
    "        self.assertEqual(len(dataset), 3)\n",
    "    \n",
    "    def test_dataset_getitem(self):\n",
    "        \"\"\"Test dataset __getitem__ returns correct format.\"\"\"\n",
    "        dataset = H5SequenceDataset(\n",
    "            [self.mol_file],\n",
    "            [self.feat_file],\n",
    "            self.tokenizer,\n",
    "            self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=32\n",
    "        )\n",
    "        \n",
    "        input_ids, attention_mask, targets = dataset[0]\n",
    "        \n",
    "        # Check types\n",
    "        self.assertIsInstance(input_ids, torch.Tensor)\n",
    "        self.assertIsInstance(attention_mask, torch.Tensor)\n",
    "        self.assertIsInstance(targets, dict)\n",
    "        \n",
    "        # Check shapes\n",
    "        self.assertEqual(input_ids.shape, (32,))\n",
    "        self.assertEqual(attention_mask.shape, (32,))\n",
    "        \n",
    "        # Check targets\n",
    "        self.assertIn(\"dimension\", targets)\n",
    "        self.assertIn(\"ring_count\", targets)\n",
    "        self.assertIn(\"chirality\", targets)\n",
    "    \n",
    "    def test_dataset_padding(self):\n",
    "        \"\"\"Test sequences are properly padded.\"\"\"\n",
    "        dataset = H5SequenceDataset(\n",
    "            [self.mol_file],\n",
    "            [self.feat_file],\n",
    "            self.tokenizer,\n",
    "            self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=32\n",
    "        )\n",
    "        \n",
    "        input_ids, attention_mask, _ = dataset[0]\n",
    "        \n",
    "        # Check BOS at start\n",
    "        self.assertEqual(input_ids[0].item(), 2)\n",
    "        \n",
    "        # Check padding\n",
    "        pad_id = self.tokenizer.token_to_id[\"<pad>\"]\n",
    "        has_padding = (input_ids == pad_id).any()\n",
    "        self.assertTrue(has_padding)\n",
    "        \n",
    "        # Check attention mask excludes padding\n",
    "        n_valid = attention_mask.sum().item()\n",
    "        n_pad = (input_ids == pad_id).sum().item()\n",
    "        self.assertEqual(n_valid + n_pad, 32)\n",
    "    \n",
    "    def test_collate_fn(self):\n",
    "        \"\"\"Test collate function properly batches data.\"\"\"\n",
    "        dataset = H5SequenceDataset(\n",
    "            [self.mol_file],\n",
    "            [self.feat_file],\n",
    "            self.tokenizer,\n",
    "            self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=32\n",
    "        )\n",
    "        \n",
    "        batch = [dataset[i] for i in range(2)]\n",
    "        input_ids, attention_masks, targets = collate_fn(batch)\n",
    "        \n",
    "        # Check batch dimensions\n",
    "        self.assertEqual(input_ids.shape, (2, 32))\n",
    "        self.assertEqual(attention_masks.shape, (2, 32))\n",
    "        \n",
    "        # Check targets are batched\n",
    "        self.assertEqual(targets[\"dimension\"].shape, (2,))\n",
    "        self.assertIsInstance(targets, dict)\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        for f in [self.mol_file, self.feat_file, self.underrep_file, self.vocab_file]:\n",
    "            if os.path.exists(f):\n",
    "                os.remove(f)\n",
    "        os.rmdir(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestModelComponents(unittest.TestCase):\n",
    "    \"\"\"Test individual model components.\"\"\"\n",
    "    \n",
    "    def test_positional_encoding(self):\n",
    "        \"\"\"Test learned positional encoding.\"\"\"\n",
    "        d_model = 64\n",
    "        max_len = 128\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        pos_enc = LearnedPositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Create dummy input\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        output = pos_enc(x)\n",
    "        \n",
    "        # Check shape preserved\n",
    "        self.assertEqual(output.shape, x.shape)\n",
    "        \n",
    "        # Check output is different from input (positions added)\n",
    "        self.assertFalse(torch.allclose(output, x))\n",
    "    \n",
    "    def test_positional_encoding_exceeds_max_len(self):\n",
    "        \"\"\"Test positional encoding raises error for sequences too long.\"\"\"\n",
    "        pos_enc = LearnedPositionalEncoding(d_model=64, max_len=32)\n",
    "        x = torch.randn(2, 64, 64)  # seq_len=64 > max_len=32\n",
    "        \n",
    "        with self.assertRaises(ValueError):\n",
    "            pos_enc(x)\n",
    "    \n",
    "    def test_cross_attention(self):\n",
    "        \"\"\"Test cross-attention module.\"\"\"\n",
    "        d_model = 64\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        cross_attn = CrossAttention(d_model=d_model, nhead=4)\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        memory = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        output = cross_attn(x, memory)\n",
    "        \n",
    "        # Check shape preserved\n",
    "        self.assertEqual(output.shape, x.shape)\n",
    "    \n",
    "    def test_cross_attention_with_mask(self):\n",
    "        \"\"\"Test cross-attention with padding mask.\"\"\"\n",
    "        d_model = 64\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        cross_attn = CrossAttention(d_model=d_model, nhead=4)\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        memory = torch.randn(batch_size, seq_len, d_model)\n",
    "        memory_mask = torch.ones(batch_size, seq_len).bool()\n",
    "        memory_mask[:, 16:] = False  # Mask second half\n",
    "        \n",
    "        output = cross_attn(x, memory, memory_mask=memory_mask)\n",
    "        \n",
    "        self.assertEqual(output.shape, x.shape)\n",
    "    \n",
    "    def test_property_block_classification(self):\n",
    "        \"\"\"Test property block for classification task.\"\"\"\n",
    "        d_model = 64\n",
    "        num_classes = 5\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        prop_block = PropertyBlock(\n",
    "            d_model=d_model,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            task=\"classification\",\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        attn_mask = torch.ones(batch_size, seq_len)\n",
    "        \n",
    "        logits, pred_emb = prop_block(x, attn_mask)\n",
    "        \n",
    "        # Check logits shape\n",
    "        self.assertEqual(logits.shape, (batch_size, num_classes))\n",
    "        \n",
    "        # Check prediction embedding shape\n",
    "        self.assertEqual(pred_emb.shape, (batch_size, seq_len, d_model))\n",
    "    \n",
    "    def test_property_block_regression(self):\n",
    "        \"\"\"Test property block for regression task.\"\"\"\n",
    "        d_model = 64\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        prop_block = PropertyBlock(\n",
    "            d_model=d_model,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            task=\"regression\"\n",
    "        )\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        attn_mask = torch.ones(batch_size, seq_len)\n",
    "        \n",
    "        logits, pred_emb = prop_block(x, attn_mask)\n",
    "        \n",
    "        # Check logits shape (regression outputs single value)\n",
    "        self.assertEqual(logits.shape, (batch_size, 1))\n",
    "        \n",
    "        # Check prediction embedding shape\n",
    "        self.assertEqual(pred_emb.shape, (batch_size, seq_len, d_model))\n",
    "    \n",
    "    def test_property_block_with_previous_memory(self):\n",
    "        \"\"\"Test property block with cross-attention to previous property.\"\"\"\n",
    "        d_model = 64\n",
    "        batch_size = 4\n",
    "        seq_len = 32\n",
    "        \n",
    "        prop_block = PropertyBlock(\n",
    "            d_model=d_model,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            task=\"classification\",\n",
    "            num_classes=3\n",
    "        )\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        attn_mask = torch.ones(batch_size, seq_len)\n",
    "        prev_memory = torch.randn(batch_size, seq_len, d_model)\n",
    "        prev_mask = torch.ones(batch_size, seq_len)\n",
    "        \n",
    "        logits, pred_emb = prop_block(x, attn_mask, prev_memory, prev_mask)\n",
    "        \n",
    "        self.assertEqual(logits.shape, (batch_size, 3))\n",
    "        self.assertEqual(pred_emb.shape, (batch_size, seq_len, d_model))\n",
    "\n",
    "\n",
    "class TestHierarchicalTransformer(unittest.TestCase):\n",
    "    \"\"\"Test full hierarchical transformer model.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up model configuration.\"\"\"\n",
    "        self.vocab_size = 100\n",
    "        self.max_len = 64\n",
    "        self.d_model = 128\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        self.property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 2},\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 2},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "        ]\n",
    "    \n",
    "    def test_model_initialization(self):\n",
    "        \"\"\"Test model initializes without errors.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model,\n",
    "            n_initial_blocks=4\n",
    "        )\n",
    "        \n",
    "        # Check model has correct number of property blocks\n",
    "        self.assertEqual(len(model.properties), 3)\n",
    "    \n",
    "    def test_model_forward(self):\n",
    "        \"\"\"Test forward pass produces correct output shapes.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model,\n",
    "            n_initial_blocks=4\n",
    "        )\n",
    "        \n",
    "        # Create dummy input\n",
    "        input_ids = torch.randint(0, self.vocab_size, (self.batch_size, self.max_len))\n",
    "        attention_mask = torch.ones(self.batch_size, self.max_len)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Check outputs\n",
    "        self.assertIsInstance(outputs, dict)\n",
    "        self.assertEqual(len(outputs), 3)\n",
    "        \n",
    "        # Check each output shape\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (self.batch_size, 5))  # classification\n",
    "        self.assertEqual(outputs[\"prop_1\"].shape, (self.batch_size, 3))  # classification\n",
    "        self.assertEqual(outputs[\"prop_2\"].shape, (self.batch_size, 1))  # regression\n",
    "    \n",
    "    def test_model_with_padding(self):\n",
    "        \"\"\"Test model handles padded sequences correctly.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model,\n",
    "            pad_idx=0\n",
    "        )\n",
    "        \n",
    "        # Create input with padding\n",
    "        input_ids = torch.randint(1, self.vocab_size, (self.batch_size, self.max_len))\n",
    "        input_ids[:, 32:] = 0  # Pad second half\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Should still produce valid outputs\n",
    "        self.assertEqual(len(outputs), 3)\n",
    "    \n",
    "    def test_model_gradient_flow(self):\n",
    "        \"\"\"Test gradients flow through entire model.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.randint(1, self.vocab_size, (self.batch_size, self.max_len))  # Avoid padding idx\n",
    "        attention_mask = torch.ones(self.batch_size, self.max_len)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Compute dummy loss\n",
    "        loss = outputs[\"prop_0\"].sum() + outputs[\"prop_1\"].sum() + outputs[\"prop_2\"].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradients exist for non-padding parameters\n",
    "        has_gradients = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                has_gradients = True\n",
    "                break\n",
    "        \n",
    "        self.assertTrue(has_gradients, \"No gradients found in model parameters\")\n",
    "    \n",
    "    def test_model_parameter_count(self):\n",
    "        \"\"\"Test model has reasonable number of parameters.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model,\n",
    "            n_initial_blocks=2\n",
    "        )\n",
    "        \n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Should have > 0 parameters\n",
    "        self.assertGreater(n_params, 0)\n",
    "        \n",
    "        # Sanity check: not unreasonably large\n",
    "        self.assertLess(n_params, 100_000_000)  # < 100M params\n",
    "\n",
    "\n",
    "class TestTrainingFunctions(unittest.TestCase):\n",
    "    \"\"\"Test training and evaluation functions.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up dummy model and data.\"\"\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.vocab_size = 50\n",
    "        self.max_len = 32\n",
    "        self.d_model = 64\n",
    "        self.batch_size = 4\n",
    "        \n",
    "        self.property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 2, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "        ]\n",
    "        \n",
    "        self.model = HierarchicalTransformer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            property_configs=self.property_configs,\n",
    "            max_len=self.max_len,\n",
    "            d_model=self.d_model,\n",
    "            n_initial_blocks=2\n",
    "        )\n",
    "        \n",
    "        self.property_info = [\n",
    "            (\"classification\", 3),\n",
    "            (\"classification\", 4),\n",
    "            (\"classification\", 2),\n",
    "            (\"classification\", 5),\n",
    "            (\"classification\", 5),\n",
    "            (\"regression\", 1),\n",
    "            (\"regression\", 1),\n",
    "        ]\n",
    "    \n",
    "    def create_dummy_dataloader(self, n_batches=5):\n",
    "        \"\"\"Create dummy dataloader for testing.\"\"\"\n",
    "        class DummyDataset:\n",
    "            def __init__(self, n_samples):\n",
    "                self.n_samples = n_samples\n",
    "            \n",
    "            def __len__(self):\n",
    "                return self.n_samples\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                input_ids = torch.randint(0, 50, (32,))\n",
    "                attention_mask = torch.ones(32)\n",
    "                targets = {\n",
    "                    \"dimension\": torch.randint(0, 3, (1,)).item(),\n",
    "                    \"ring_count\": torch.randint(0, 4, (1,)).item(),\n",
    "                    \"chirality\": torch.randint(0, 2, (1,)).item(),\n",
    "                    \"n_symmetry_planes\": torch.randint(0, 5, (1,)).item(),\n",
    "                    \"point_group\": torch.randint(0, 5, (1,)).item(),\n",
    "                    \"planar_fit_error\": torch.randn(1).item(),\n",
    "                    \"ring_plane_angles\": torch.randint(0, 5, (1,)).item(),\n",
    "                }\n",
    "                return input_ids, attention_mask, targets\n",
    "        \n",
    "        def collate(batch):\n",
    "            input_ids = torch.stack([b[0] for b in batch])\n",
    "            attention_masks = torch.stack([b[1] for b in batch])\n",
    "            targets = {\n",
    "                \"dimension\": torch.tensor([b[2][\"dimension\"] for b in batch]),\n",
    "                \"ring_count\": torch.tensor([b[2][\"ring_count\"] for b in batch]),\n",
    "                \"chirality\": torch.tensor([b[2][\"chirality\"] for b in batch]),\n",
    "                \"n_symmetry_planes\": torch.tensor([b[2][\"n_symmetry_planes\"] for b in batch]),\n",
    "                \"point_group\": torch.tensor([b[2][\"point_group\"] for b in batch]),\n",
    "                \"planar_fit_error\": torch.tensor([b[2][\"planar_fit_error\"] for b in batch]),\n",
    "                \"ring_plane_angles\": torch.tensor([b[2][\"ring_plane_angles\"] for b in batch]),\n",
    "            }\n",
    "            return input_ids, attention_masks, targets\n",
    "        \n",
    "        from torch.utils.data import DataLoader\n",
    "        dataset = DummyDataset(n_batches * self.batch_size)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, collate_fn=collate)\n",
    "        \n",
    "    def test_train_one_epoch(self):\n",
    "        \"\"\"Test training for one epoch runs without errors.\"\"\"\n",
    "        dataloader = self.create_dummy_dataloader()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        \n",
    "        loss = train_one_epoch(\n",
    "            self.model,\n",
    "            dataloader,\n",
    "            optimizer,\n",
    "            self.device,\n",
    "            self.property_info,\n",
    "            bf16=False\n",
    "        )\n",
    "        \n",
    "        # Check loss is a valid number\n",
    "        self.assertIsInstance(loss, float)\n",
    "        self.assertGreater(loss, 0)\n",
    "    \n",
    "    def test_evaluate(self):\n",
    "        \"\"\"Test evaluation runs without errors.\"\"\"\n",
    "        dataloader = self.create_dummy_dataloader()\n",
    "        \n",
    "        mse, rmse = evaluate(\n",
    "            self.model,\n",
    "            dataloader,\n",
    "            self.device,\n",
    "            self.property_info,\n",
    "            bf16=False\n",
    "        )\n",
    "        \n",
    "        # Check metrics are valid\n",
    "        self.assertIsInstance(mse, float)\n",
    "        self.assertIsInstance(rmse, float)\n",
    "        self.assertGreater(mse, 0)\n",
    "        self.assertGreater(rmse, 0)\n",
    "        self.assertAlmostEqual(rmse, mse ** 0.5, places=5)\n",
    "    \n",
    "    def test_training_updates_parameters(self):\n",
    "        \"\"\"Test that training actually updates model parameters.\"\"\"\n",
    "        dataloader = self.create_dummy_dataloader(n_batches=2)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        \n",
    "        # Store initial parameters\n",
    "        initial_params = {\n",
    "            name: param.clone()\n",
    "            for name, param in self.model.named_parameters()\n",
    "        }\n",
    "        \n",
    "        # Train\n",
    "        train_one_epoch(\n",
    "            self.model,\n",
    "            dataloader,\n",
    "            optimizer,\n",
    "            self.device,\n",
    "            self.property_info,\n",
    "            bf16=False\n",
    "        )\n",
    "        \n",
    "        # Check at least some parameters changed\n",
    "        changed = False\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not torch.allclose(param, initial_params[name]):\n",
    "                changed = True\n",
    "                break\n",
    "        \n",
    "        self.assertTrue(changed, \"No parameters were updated during training\")\n",
    "    \n",
    "    def test_evaluation_no_gradient(self):\n",
    "        \"\"\"Test evaluation doesn't compute gradients.\"\"\"\n",
    "        dataloader = self.create_dummy_dataloader()\n",
    "        \n",
    "        # Ensure model is in eval mode and no_grad is respected\n",
    "        with torch.no_grad():\n",
    "            mse, rmse = evaluate(\n",
    "                self.model,\n",
    "                dataloader,\n",
    "                self.device,\n",
    "                self.property_info,\n",
    "                bf16=False\n",
    "            )\n",
    "        \n",
    "        # Check no gradients are stored\n",
    "        for param in self.model.parameters():\n",
    "            self.assertIsNone(param.grad)\n",
    "\n",
    "\n",
    "class TestIntegration(unittest.TestCase):\n",
    "    \"\"\"Integration tests for end-to-end workflows.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up complete pipeline components.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Create vocab\n",
    "        self.vocab_file = os.path.join(self.temp_dir, \"vocab.json\")\n",
    "        vocab_data = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"=\", \"(\", \")\"],\n",
    "            \"token_to_id\": {tok: i for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"=\", \"(\", \")\"]\n",
    "            )},\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"=\", \"(\", \")\"]\n",
    "            )}\n",
    "        }\n",
    "        with open(self.vocab_file, \"w\") as f:\n",
    "            json.dump(vocab_data, f)\n",
    "        \n",
    "        # Create HDF5 files\n",
    "        self.mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        with h5py.File(self.mol_file, \"w\") as f:\n",
    "            smiles = [b\"CCO\", b\"CC(C)O\", b\"C=O\", b\"CC=O\", b\"CCC\"]\n",
    "            f.create_dataset(\"smiles\", data=np.array(smiles, dtype=\"S\"))\n",
    "        \n",
    "        self.feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        with h5py.File(self.feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\", b\"tetrahedral\", b\"planar\", b\"planar\", b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\", b\"Cs\", b\"C2v\", b\"Cs\", b\"D3h\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0, 1, 2, 1, 3], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0, 1, 0, 0, 0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0, 0, 0, 0, 0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1, 0.2, 0.15, 0.18, 0.12], dtype=float))\n",
    "            \n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            angles = np.array([], dtype=dt)\n",
    "            f.create_dataset(\"plane_angles\", data=angles)\n",
    "        \n",
    "        # Create underrepresented data\n",
    "        self.underrep_file = os.path.join(self.temp_dir, \"underrep.json\")\n",
    "        with open(self.underrep_file, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"point_groups\": [\"rare\"],\n",
    "                \"symmetry_planes\": 5,\n",
    "                \"nrings\": 3\n",
    "            }, f)\n",
    "    \n",
    "    def test_full_pipeline(self):\n",
    "        \"\"\"Test complete pipeline: data loading → model → training → evaluation.\"\"\"\n",
    "        # Initialize tokenizer\n",
    "        tokenizer = SmilesTokenizer(self.vocab_file)\n",
    "        \n",
    "        # Initialize label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(\"dimension\", [\"linear\", \"planar\", \"tetrahedral\"])\n",
    "        label_encoder.fit(\"ring_count\", [\"0\", \"1\", \"2\", \"3+\"])\n",
    "        label_encoder.fit(\"n_symmetry_planes\", [\"0\", \"1\", \"2\", \"3\", \"5+\"])\n",
    "        label_encoder.fit(\"point_group\", [\"C1\", \"Cs\", \"C2v\", \"D3h\", \"Other\"])\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = H5SequenceDataset(\n",
    "            [self.mol_file],\n",
    "            [self.feat_file],\n",
    "            tokenizer,\n",
    "            label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=32\n",
    "        )\n",
    "        \n",
    "        # Create dataloader\n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        # Create model\n",
    "        property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 2, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            property_configs=property_configs,\n",
    "            max_len=32,\n",
    "            d_model=64,\n",
    "            n_initial_blocks=2,\n",
    "            pad_idx=tokenizer.token_to_id[\"<pad>\"]\n",
    "        )\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        property_info = [(cfg[\"task\"], cfg.get(\"num_classes\", 1)) for cfg in property_configs]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        # Train one epoch\n",
    "        train_loss = train_one_epoch(model, dataloader, optimizer, device, property_info, bf16=False)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_mse, val_rmse = evaluate(model, dataloader, device, property_info, bf16=False)\n",
    "        \n",
    "        # Check results are valid\n",
    "        self.assertIsInstance(train_loss, float)\n",
    "        self.assertGreater(train_loss, 0)\n",
    "        self.assertIsInstance(val_rmse, float)\n",
    "        self.assertGreater(val_rmse, 0)\n",
    "    \n",
    "    def test_overfitting_simple_data(self):\n",
    "        \"\"\"Test model can overfit to small dataset (sanity check).\"\"\"\n",
    "        # Initialize components\n",
    "        tokenizer = SmilesTokenizer(self.vocab_file)\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(\"dimension\", [\"linear\", \"planar\"])\n",
    "        label_encoder.fit(\"ring_count\", [\"0\", \"1\", \"4+\"])\n",
    "        label_encoder.fit(\"n_symmetry_planes\", [\"0\", \"1\", \"2\", \"5+\"])\n",
    "        label_encoder.fit(\"point_group\", [\"C1\", \"Cs\", \"C2v\", \"Other\"])\n",
    "        \n",
    "        # Create small dataset (just 2 samples)\n",
    "        mol_file_small = os.path.join(self.temp_dir, \"mols_small.h5\")\n",
    "        with h5py.File(mol_file_small, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"CCO\", b\"C=O\"], dtype=\"S\"))\n",
    "        \n",
    "        feat_file_small = os.path.join(self.temp_dir, \"feats_small.h5\")\n",
    "        with h5py.File(feat_file_small, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\", b\"planar\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\", b\"Cs\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0, 1], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0, 0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0, 0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1, 0.2], dtype=float))\n",
    "            \n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            angles = np.array([], dtype=dt)\n",
    "            f.create_dataset(\"plane_angles\", data=angles)\n",
    "        \n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file_small],\n",
    "            [feat_file_small],\n",
    "            tokenizer,\n",
    "            label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file,\n",
    "            mode=\"smiles\",\n",
    "            max_len=16\n",
    "        )\n",
    "        \n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        # Small model - FIXED: num_classes must be >= 2 for ring_count which has 3 classes\n",
    "        property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 2, \"n_blocks\": 1},   # dimension\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},   # ring_count (FIX: was 1, should be 3)\n",
    "            {\"task\": \"classification\", \"num_classes\": 2, \"n_blocks\": 1},   # chirality\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 1},   # n_symmetry_planes (FIX: was 2, should be 4)\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 1},   # point_group (FIX: was 2, should be 4)\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},                          # planar_fit_error\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},                          # ring_plane_angles\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            property_configs=property_configs,\n",
    "            max_len=16,\n",
    "            d_model=32,\n",
    "            n_initial_blocks=1,\n",
    "            pad_idx=tokenizer.token_to_id[\"<pad>\"]\n",
    "        )\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        property_info = [(cfg[\"task\"], cfg.get(\"num_classes\", 1)) for cfg in property_configs]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "        \n",
    "        # Train multiple epochs\n",
    "        initial_loss = None\n",
    "        final_loss = None\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            loss = train_one_epoch(model, dataloader, optimizer, device, property_info, bf16=False)\n",
    "            if epoch == 0:\n",
    "                initial_loss = loss\n",
    "            if epoch == 49:\n",
    "                final_loss = loss\n",
    "        \n",
    "        # Loss should decrease (overfitting to small dataset)\n",
    "        self.assertLess(final_loss, initial_loss, \"Model should overfit to small dataset\")\n",
    "    \n",
    "    def test_model_save_load(self):\n",
    "        \"\"\"Test saving and loading model checkpoint.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.vocab_file)\n",
    "        \n",
    "        property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            property_configs=property_configs,\n",
    "            max_len=32,\n",
    "            d_model=64,\n",
    "            n_initial_blocks=2,\n",
    "            pad_idx=0\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(self.temp_dir, \"checkpoint.pt\")\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"property_configs\": property_configs,\n",
    "            \"vocab_size\": tokenizer.vocab_size,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "        \n",
    "        # Create new model and load state\n",
    "        model_loaded = HierarchicalTransformer(\n",
    "            vocab_size=checkpoint[\"vocab_size\"],\n",
    "            property_configs=checkpoint[\"property_configs\"],\n",
    "            max_len=32,\n",
    "            d_model=64,\n",
    "            n_initial_blocks=2,\n",
    "            pad_idx=0\n",
    "        )\n",
    "        model_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        \n",
    "        # Set both to eval mode to disable dropout\n",
    "        model.eval()\n",
    "        model_loaded.eval()\n",
    "        \n",
    "        # Test both models produce same output\n",
    "        input_ids = torch.randint(0, tokenizer.vocab_size, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out1 = model(input_ids, attention_mask)\n",
    "            out2 = model_loaded(input_ids, attention_mask)\n",
    "        \n",
    "        # Check outputs match\n",
    "        for key in out1.keys():\n",
    "            self.assertTrue(torch.allclose(out1[key], out2[key]), f\"Output {key} doesn't match after reload\")\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.temp_dir):\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestEdgeCases(unittest.TestCase):\n",
    "    \"\"\"Test edge cases and error handling.\"\"\"\n",
    "    \n",
    "    def test_empty_sequence(self):\n",
    "        \"\"\"Test handling of empty sequences.\"\"\"\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        try:\n",
    "            # Create vocab\n",
    "            vocab_file = os.path.join(temp_dir, \"vocab.json\")\n",
    "            vocab_data = {\n",
    "                \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"],\n",
    "                \"token_to_id\": {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3},\n",
    "                \"id_to_token\": {\"0\": \"<pad>\", \"1\": \"<unk>\", \"2\": \"<bos>\", \"3\": \"<eos>\"}\n",
    "            }\n",
    "            with open(vocab_file, \"w\") as f:\n",
    "                json.dump(vocab_data, f)\n",
    "            \n",
    "            tokenizer = SmilesTokenizer(vocab_file)\n",
    "            \n",
    "            # Empty string\n",
    "            ids = tokenizer.encode(\"\", add_special=True)\n",
    "            self.assertEqual(len(ids), 2)  # Just BOS and EOS\n",
    "            self.assertEqual(ids[0], 2)  # BOS\n",
    "            self.assertEqual(ids[1], 3)  # EOS\n",
    "            \n",
    "        finally:\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    def test_very_long_sequence(self):\n",
    "        \"\"\"Test truncation of very long sequences.\"\"\"\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        try:\n",
    "            vocab_file = os.path.join(temp_dir, \"vocab.json\")\n",
    "            vocab_data = {\n",
    "                \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\"],\n",
    "                \"token_to_id\": {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3, \"C\": 4},\n",
    "                \"id_to_token\": {\"0\": \"<pad>\", \"1\": \"<unk>\", \"2\": \"<bos>\", \"3\": \"<eos>\", \"4\": \"C\"}\n",
    "            }\n",
    "            with open(vocab_file, \"w\") as f:\n",
    "                json.dump(vocab_data, f)\n",
    "            \n",
    "            tokenizer = SmilesTokenizer(vocab_file)\n",
    "            \n",
    "            # Very long sequence\n",
    "            long_seq = \"C\" * 1000\n",
    "            ids = tokenizer.encode(long_seq, add_special=False)\n",
    "            \n",
    "            # Should tokenize all characters\n",
    "            self.assertEqual(len(ids), 1000)\n",
    "            \n",
    "        finally:\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_dir)\n",
    "    \n",
    "    def test_invalid_property_config(self):\n",
    "        \"\"\"Test model rejects invalid property configurations.\"\"\"\n",
    "        with self.assertRaises((ValueError, AssertionError)):\n",
    "            # Classification without num_classes\n",
    "            PropertyBlock(\n",
    "                d_model=64,\n",
    "                task=\"classification\",\n",
    "                num_classes=None  # Should raise error\n",
    "            )\n",
    "    \n",
    "    def test_mismatched_batch_sizes(self):\n",
    "        \"\"\"Test error handling for mismatched batch sizes.\"\"\"\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=100,\n",
    "            property_configs=[{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}],\n",
    "            max_len=32,\n",
    "            d_model=64\n",
    "        )\n",
    "        \n",
    "        # Mismatched batch sizes should raise error\n",
    "        input_ids = torch.randint(0, 100, (4, 32))\n",
    "        attention_mask = torch.ones(8, 32)  # Different batch size\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            model(input_ids, attention_mask)\n",
    "            \n",
    "class TestDatasetMethods(unittest.TestCase):\n",
    "    \"\"\"Test specific H5SequenceDataset methods not covered in basic tests.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        self.vocab_file = os.path.join(self.temp_dir, \"vocab.json\")\n",
    "        vocab_data = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\"],\n",
    "            \"token_to_id\": {\"<pad>\": 0, \"<unk>\": 1, \"<bos>\": 2, \"<eos>\": 3, \"C\": 4},\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate([\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\"])}\n",
    "        }\n",
    "        with open(self.vocab_file, \"w\") as f:\n",
    "            json.dump(vocab_data, f)\n",
    "        \n",
    "        self.tokenizer = SmilesTokenizer(self.vocab_file)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(\"dimension\", [\"linear\"])\n",
    "        self.label_encoder.fit(\"ring_count\", [\"0\", \"1\", \"2\", \"3\", \"4\", \"5+\"])\n",
    "        self.label_encoder.fit(\"n_symmetry_planes\", [\"0\", \"1\", \"2\", \"3\", \"4\", \"5+\"])\n",
    "        self.label_encoder.fit(\"point_group\", [\"C1\", \"Other\"])\n",
    "        \n",
    "        self.underrep_file = os.path.join(self.temp_dir, \"underrep.json\")\n",
    "        with open(self.underrep_file, \"w\") as f:\n",
    "            json.dump({\"point_groups\": [], \"symmetry_planes\": 5, \"nrings\": 5}, f)\n",
    "    \n",
    "    def test_assign_binned_labels_below_threshold(self):\n",
    "        \"\"\"Test assign_binned_labels for values below threshold.\"\"\"\n",
    "        mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        \n",
    "        with h5py.File(mol_file, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"C\"], dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([3], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([2], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1], dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file], [feat_file], self.tokenizer, self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file, mode=\"smiles\", max_len=16\n",
    "        )\n",
    "        \n",
    "        # Test binning below threshold\n",
    "        result = dataset.assign_binned_labels(3, 5)\n",
    "        self.assertEqual(result, \"3\")\n",
    "    \n",
    "    def test_assign_binned_labels_above_threshold(self):\n",
    "        \"\"\"Test assign_binned_labels for values at or above threshold.\"\"\"\n",
    "        mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        \n",
    "        with h5py.File(mol_file, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"C\"], dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1], dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file], [feat_file], self.tokenizer, self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file, mode=\"smiles\", max_len=16\n",
    "        )\n",
    "        \n",
    "        # Test binning at threshold\n",
    "        result = dataset.assign_binned_labels(5, 5)\n",
    "        self.assertEqual(result, \"5+\")\n",
    "        \n",
    "        # Test binning above threshold\n",
    "        result = dataset.assign_binned_labels(10, 5)\n",
    "        self.assertEqual(result, \"5+\")\n",
    "    \n",
    "    def test_assign_other_label(self):\n",
    "        \"\"\"Test assign_other_label method.\"\"\"\n",
    "        mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        \n",
    "        with h5py.File(mol_file, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"C\"], dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1], dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file], [feat_file], self.tokenizer, self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file, mode=\"smiles\", max_len=16\n",
    "        )\n",
    "        \n",
    "        # Test label in change list -> should become \"Other\"\n",
    "        result = dataset.assign_other_label(\"rare_pg\", [\"rare_pg\", \"another_rare\"])\n",
    "        self.assertEqual(result, \"Other\")\n",
    "        \n",
    "        # Test label not in change list -> should stay same\n",
    "        result = dataset.assign_other_label(\"C2v\", [\"rare_pg\"])\n",
    "        self.assertEqual(result, \"C2v\")\n",
    "        \n",
    "        # Test custom new_label\n",
    "        result = dataset.assign_other_label(\"rare_pg\", [\"rare_pg\"], \"Unknown\")\n",
    "        self.assertEqual(result, \"Unknown\")\n",
    "    \n",
    "    def test_dataset_with_max_molecules(self):\n",
    "        \"\"\"Test dataset with max_molecules parameter.\"\"\"\n",
    "        mol_file = os.path.join(self.temp_dir, \"mols.h5\")\n",
    "        feat_file = os.path.join(self.temp_dir, \"feats.h5\")\n",
    "        \n",
    "        with h5py.File(mol_file, \"w\") as f:\n",
    "            smiles = [b\"C\", b\"CC\", b\"CCC\", b\"CCCC\", b\"CCCCC\"]\n",
    "            f.create_dataset(\"smiles\", data=np.array(smiles, dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"]*5, dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"]*5, dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0]*5, dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0]*5, dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0]*5, dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1]*5, dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        # Create dataset with max_molecules=3\n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file], [feat_file], self.tokenizer, self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file, mode=\"smiles\", \n",
    "            max_len=16, max_molecules=3\n",
    "        )\n",
    "        \n",
    "        # Should only load 3 molecules\n",
    "        self.assertEqual(len(dataset), 3)\n",
    "    \n",
    "    def test_dataset_multi_file_handling(self):\n",
    "        \"\"\"Test dataset with multiple HDF5 files.\"\"\"\n",
    "        mol_file1 = os.path.join(self.temp_dir, \"mols1.h5\")\n",
    "        mol_file2 = os.path.join(self.temp_dir, \"mols2.h5\")\n",
    "        feat_file1 = os.path.join(self.temp_dir, \"feats1.h5\")\n",
    "        feat_file2 = os.path.join(self.temp_dir, \"feats2.h5\")\n",
    "        \n",
    "        # Create first file pair\n",
    "        with h5py.File(mol_file1, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"C\", b\"CC\"], dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file1, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"]*2, dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"]*2, dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0]*2, dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0]*2, dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0]*2, dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1]*2, dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        # Create second file pair\n",
    "        with h5py.File(mol_file2, \"w\") as f:\n",
    "            f.create_dataset(\"smiles\", data=np.array([b\"CCC\"], dtype=\"S\"))\n",
    "        \n",
    "        with h5py.File(feat_file2, \"w\") as f:\n",
    "            f.create_dataset(\"dimensions\", data=np.array([b\"linear\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"point_groups\", data=np.array([b\"C1\"], dtype=\"S\"))\n",
    "            f.create_dataset(\"symmetry_planes\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"chiralities\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"nrings\", data=np.array([0], dtype=int))\n",
    "            f.create_dataset(\"errors\", data=np.array([0.1], dtype=float))\n",
    "            dt = np.dtype([(\"i\", \"i4\"), (\"j\", \"i4\"), (\"val\", \"f4\")])\n",
    "            f.create_dataset(\"plane_angles\", data=np.array([], dtype=dt))\n",
    "        \n",
    "        # Create dataset with multiple files\n",
    "        dataset = H5SequenceDataset(\n",
    "            [mol_file1, mol_file2], [feat_file1, feat_file2], \n",
    "            self.tokenizer, self.label_encoder,\n",
    "            underrepresented_data_file=self.underrep_file, mode=\"smiles\", max_len=16\n",
    "        )\n",
    "        \n",
    "        # Should load all molecules from both files\n",
    "        self.assertEqual(len(dataset), 3)\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.temp_dir):\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestTokenizerEdgeCases(unittest.TestCase):\n",
    "    \"\"\"Test edge cases for tokenizers.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Create temporary vocab files.\"\"\"\n",
    "        self.temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Extended SMILES vocab with special characters\n",
    "        smiles_vocab = {\n",
    "            \"tokens\": [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"Cl\", \"Br\",\n",
    "                      \"[nH]\", \"[C@@H]\", \"@\", \"@@\", \"#\", \"%10\", \"/\", \"\\\\\"],\n",
    "            \"token_to_id\": {tok: i for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"Cl\", \"Br\",\n",
    "                 \"[nH]\", \"[C@@H]\", \"@\", \"@@\", \"#\", \"%10\", \"/\", \"\\\\\"]\n",
    "            )},\n",
    "            \"id_to_token\": {str(i): tok for i, tok in enumerate(\n",
    "                [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"C\", \"N\", \"O\", \"Cl\", \"Br\",\n",
    "                 \"[nH]\", \"[C@@H]\", \"@\", \"@@\", \"#\", \"%10\", \"/\", \"\\\\\"]\n",
    "            )}\n",
    "        }\n",
    "        \n",
    "        self.smiles_vocab_file = os.path.join(self.temp_dir, \"smiles_vocab.json\")\n",
    "        with open(self.smiles_vocab_file, \"w\") as f:\n",
    "            json.dump(smiles_vocab, f)\n",
    "    \n",
    "    def test_smiles_bracketed_atoms(self):\n",
    "        \"\"\"Test SMILES tokenization of bracketed atoms.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Test bracketed expression\n",
    "        tokens = tokenizer.tokenize(\"C[nH]C\")\n",
    "        self.assertIn(\"[nH]\", tokens)\n",
    "        self.assertEqual(len(tokens), 3)  # C, [nH], C\n",
    "    \n",
    "    def test_smiles_stereochemistry(self):\n",
    "        \"\"\"Test SMILES tokenization of stereochemistry markers.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Single @\n",
    "        tokens = tokenizer.tokenize(\"C@C\")\n",
    "        self.assertIn(\"@\", tokens)\n",
    "        \n",
    "        # Double @@\n",
    "        tokens = tokenizer.tokenize(\"C@@C\")\n",
    "        self.assertIn(\"@@\", tokens)\n",
    "    \n",
    "    def test_smiles_two_digit_ring_closure(self):\n",
    "        \"\"\"Test SMILES tokenization of %NN ring closures.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        tokens = tokenizer.tokenize(\"C%10CC%10\")\n",
    "        self.assertIn(\"%10\", tokens)\n",
    "    \n",
    "    def test_smiles_bonds(self):\n",
    "        \"\"\"Test SMILES tokenization of different bond types.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Triple bond\n",
    "        tokens = tokenizer.tokenize(\"C#N\")\n",
    "        self.assertIn(\"#\", tokens)\n",
    "        \n",
    "        # Directional bonds\n",
    "        tokens = tokenizer.tokenize(\"C/C\")\n",
    "        self.assertIn(\"/\", tokens)\n",
    "        \n",
    "        tokens = tokenizer.tokenize(\"C\\\\C\")\n",
    "        self.assertIn(\"\\\\\", tokens)\n",
    "    \n",
    "    def test_smiles_multi_char_atoms(self):\n",
    "        \"\"\"Test SMILES tokenization of multi-character atoms.\"\"\"\n",
    "        tokenizer = SmilesTokenizer(self.smiles_vocab_file)\n",
    "        \n",
    "        # Cl should be one token\n",
    "        tokens = tokenizer.tokenize(\"CCl\")\n",
    "        self.assertEqual(tokens, [\"C\", \"Cl\"])\n",
    "        \n",
    "        # Br should be one token\n",
    "        tokens = tokenizer.tokenize(\"CBr\")\n",
    "        self.assertEqual(tokens, [\"C\", \"Br\"])\n",
    "    \n",
    "    def tearDown(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.temp_dir):\n",
    "            shutil.rmtree(self.temp_dir)\n",
    "\n",
    "\n",
    "class TestModelArchitectureVariations(unittest.TestCase):\n",
    "    \"\"\"Test model with different architectural configurations.\"\"\"\n",
    "    \n",
    "    def test_different_n_initial_blocks(self):\n",
    "        \"\"\"Test model with varying numbers of initial encoder blocks.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}]\n",
    "        \n",
    "        # Test with 0 initial blocks\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, n_initial_blocks=0\n",
    "        )\n",
    "        self.assertIsNotNone(model)\n",
    "        \n",
    "        # Test with 1 initial block\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, n_initial_blocks=1\n",
    "        )\n",
    "        self.assertIsNotNone(model)\n",
    "        \n",
    "        # Test with 8 initial blocks\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, n_initial_blocks=8\n",
    "        )\n",
    "        self.assertIsNotNone(model)\n",
    "    \n",
    "    def test_different_dim_feedforward(self):\n",
    "        \"\"\"Test model with different feedforward dimensions.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}]\n",
    "        \n",
    "        # Small FFN\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, dim_feedforward=128\n",
    "        )\n",
    "        input_ids = torch.randint(0, vocab_size, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (2, 3))\n",
    "        \n",
    "        # Large FFN\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, dim_feedforward=4096\n",
    "        )\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (2, 3))\n",
    "    \n",
    "    def test_different_num_heads(self):\n",
    "        \"\"\"Test model with different numbers of attention heads.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}]\n",
    "        \n",
    "        # 2 heads\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64, nhead=2\n",
    "        )\n",
    "        input_ids = torch.randint(0, vocab_size, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (2, 3))\n",
    "        \n",
    "        # 16 heads (d_model must be divisible by nhead, so use d_model=128)\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=128, nhead=16\n",
    "        )\n",
    "        input_ids = torch.randint(0, vocab_size, (2, 32))\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (2, 3))\n",
    "    \n",
    "    def test_all_classification_properties(self):\n",
    "        \"\"\"Test model with all classification properties.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 2, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 1},\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.randint(0, vocab_size, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        self.assertEqual(len(outputs), 4)\n",
    "        self.assertEqual(outputs[\"prop_0\"].shape, (2, 3))\n",
    "        self.assertEqual(outputs[\"prop_3\"].shape, (2, 5))\n",
    "    \n",
    "    def test_all_regression_properties(self):\n",
    "        \"\"\"Test model with all regression properties.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.randint(0, vocab_size, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        self.assertEqual(len(outputs), 3)\n",
    "        for i in range(3):\n",
    "            self.assertEqual(outputs[f\"prop_{i}\"].shape, (2, 1))\n",
    "    \n",
    "    def test_varying_n_blocks_per_property(self):\n",
    "        \"\"\"Test model with different n_blocks for each property.\"\"\"\n",
    "        vocab_size = 50\n",
    "        property_configs = [\n",
    "            {\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1},\n",
    "            {\"task\": \"classification\", \"num_classes\": 4, \"n_blocks\": 3},\n",
    "            {\"task\": \"regression\", \"n_blocks\": 2},\n",
    "        ]\n",
    "        \n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=vocab_size, property_configs=property_configs,\n",
    "            max_len=32, d_model=64\n",
    "        )\n",
    "        \n",
    "        # Check that property blocks have correct number of layers\n",
    "        self.assertEqual(len(model.properties[0].encoder.layers), 1)\n",
    "        self.assertEqual(len(model.properties[1].encoder.layers), 3)\n",
    "        self.assertEqual(len(model.properties[2].encoder.layers), 2)\n",
    "\n",
    "\n",
    "class TestErrorHandling(unittest.TestCase):\n",
    "    \"\"\"Test error handling and validation.\"\"\"\n",
    "    \n",
    "    def test_missing_vocab_file(self):\n",
    "        \"\"\"Test error when vocab file doesn't exist.\"\"\"\n",
    "        with self.assertRaises(FileNotFoundError):\n",
    "            load_vocab(\"nonexistent_vocab.json\")\n",
    "    \n",
    "    def test_invalid_task_type(self):\n",
    "        \"\"\"Test error for invalid task type in PropertyBlock.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            PropertyBlock(d_model=64, task=\"invalid_task\")\n",
    "    \n",
    "    def test_classification_without_num_classes(self):\n",
    "        \"\"\"Test error for classification task without num_classes.\"\"\"\n",
    "        with self.assertRaises(AssertionError):\n",
    "            PropertyBlock(d_model=64, task=\"classification\", num_classes=None)\n",
    "    \n",
    "    def test_model_with_mismatched_attention_heads(self):\n",
    "        \"\"\"Test error when d_model not divisible by nhead.\"\"\"\n",
    "        with self.assertRaises(AssertionError):\n",
    "            # d_model=64, nhead=5 -> 64 % 5 != 0\n",
    "            HierarchicalTransformer(\n",
    "                vocab_size=50,\n",
    "                property_configs=[{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}],\n",
    "                max_len=32,\n",
    "                d_model=64,\n",
    "                nhead=5\n",
    "            )\n",
    "    \n",
    "    def test_label_encoder_empty_property(self):\n",
    "        \"\"\"Test label encoder with property that hasn't been fit.\"\"\"\n",
    "        encoder = LabelEncoder()\n",
    "        \n",
    "        # Get num_classes for unfitted property\n",
    "        self.assertEqual(encoder.get_num_classes(\"nonexistent\"), 0)\n",
    "        \n",
    "        # Inverse transform for unfitted property\n",
    "        result = encoder.inverse_transform(\"nonexistent\", 0)\n",
    "        self.assertEqual(result, \"unknown\")\n",
    "\n",
    "\n",
    "class TestDeterminism(unittest.TestCase):\n",
    "    \"\"\"Test model determinism and reproducibility.\"\"\"\n",
    "    \n",
    "    def test_positional_encoding_determinism(self):\n",
    "        \"\"\"Test positional encoding produces same results with same seed.\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        pe1 = LearnedPositionalEncoding(d_model=64, max_len=32)\n",
    "        \n",
    "        torch.manual_seed(42)\n",
    "        pe2 = LearnedPositionalEncoding(d_model=64, max_len=32)\n",
    "        \n",
    "        # Both should have same initial weights\n",
    "        self.assertTrue(torch.allclose(pe1.pe.weight, pe2.pe.weight))\n",
    "    \n",
    "    def test_model_forward_determinism(self):\n",
    "        \"\"\"Test model produces same outputs with same inputs in eval mode.\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=50,\n",
    "            property_configs=[{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}],\n",
    "            max_len=32, d_model=64\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        input_ids = torch.randint(0, 50, (2, 32))\n",
    "        attention_mask = torch.ones(2, 32)\n",
    "        \n",
    "        # Run twice\n",
    "        with torch.no_grad():\n",
    "            out1 = model(input_ids, attention_mask)\n",
    "            out2 = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Should be identical in eval mode\n",
    "        self.assertTrue(torch.allclose(out1[\"prop_0\"], out2[\"prop_0\"]))\n",
    "    \n",
    "    def test_dropout_in_train_vs_eval(self):\n",
    "        \"\"\"Test dropout behaves differently in train vs eval mode.\"\"\"\n",
    "        torch.manual_seed(42)\n",
    "        model = HierarchicalTransformer(\n",
    "            vocab_size=50,\n",
    "            property_configs=[{\"task\": \"classification\", \"num_classes\": 3, \"n_blocks\": 1}],\n",
    "            max_len=32, d_model=64, dropout=0.5\n",
    "        )\n",
    "        \n",
    "        input_ids = torch.randint(0, 50, (4, 32))\n",
    "        attention_mask = torch.ones(4, 32)\n",
    "        \n",
    "        # Train mode - dropout active (may give different results)\n",
    "        model.train()\n",
    "        out_train1 = model(input_ids, attention_mask)\n",
    "        out_train2 = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Eval mode - dropout disabled (should give same results)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_eval1 = model(input_ids, attention_mask)\n",
    "            out_eval2 = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Eval outputs should be identical\n",
    "        self.assertTrue(torch.allclose(out_eval1[\"prop_0\"], out_eval2[\"prop_0\"]))\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run complete test suite (original + extended).\"\"\"\n",
    "    loader = unittest.TestLoader()\n",
    "    suite = unittest.TestSuite()\n",
    "    \n",
    "    # Add all test classes\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestVocabLoader))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestLabelEncoder))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestTokenizers))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDataset))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestModelComponents))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestHierarchicalTransformer))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestTrainingFunctions))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestIntegration))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestEdgeCases))\n",
    "    \n",
    "    # Add extended tests\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDatasetMethods))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestTokenizerEdgeCases))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestModelArchitectureVariations))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestErrorHandling))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDeterminism))\n",
    "    \n",
    "    # Run tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE TEST SUMMARY (ORIGINAL + EXTENDED)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Tests run: {result.testsRun}\")\n",
    "    print(f\"Successes: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
    "    print(f\"Failures: {len(result.failures)}\")\n",
    "    print(f\"Errors: {len(result.errors)}\")\n",
    "    \n",
    "    if result.wasSuccessful():\n",
    "        print(\"\\n✓ All tests passed!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Some tests failed.\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcb67f-e739-4cb3-9b6f-75e186c7dd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
