{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689509f9-8c9d-4d04-aa99-3d3418cf9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import selfies as sf\n",
    "\n",
    "# ==============================\n",
    "# 1. Tokenizers\n",
    "# ==============================\n",
    "\n",
    "# Regex for tokenizing SMILES strings\n",
    "# Matches:\n",
    "# - Single and multi-letter atoms (C, Cl, Br, etc.)\n",
    "# - Bracketed expressions (e.g., [13CH3], [O-])\n",
    "# - Stereochemistry symbols (@, @@)\n",
    "# - Bonds (=, #, +, -, /, \\)\n",
    "# - Parentheses for branching, dots, wildcards (*)\n",
    "# - Ring closures (1-9, %10-%99)\n",
    "SMILES_REGEX = re.compile(\n",
    "    r\"\"\"([A-Z][a-z]?               # single and multi-letter atoms\n",
    "        | \\[ [^\\]]+ \\]             # bracketed expressions\n",
    "        | @@?                      # stereochemistry @ / @@\n",
    "        | =|#|\\+|-|/|\\\\            # bonds\n",
    "        | \\(|\\)|\\.|\\*              # parentheses, dot, wildcard\n",
    "        | \\d                       # single-digit ring closures\n",
    "        | %\\d{2}                   # two-digit ring closures\n",
    "    )\"\"\",\n",
    "    re.X\n",
    ")\n",
    "\n",
    "def tokenize_smiles(smiles: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes a SMILES string using SMILES_REGEX.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): The SMILES string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        List of tokens with empty strings filtered out.\n",
    "    \"\"\"\n",
    "    return [t for t in SMILES_REGEX.findall(smiles) if t]\n",
    "\n",
    "def tokenize_selfies(selfies_str: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes a SELFIES string using the selfies library.\n",
    "\n",
    "    Args:\n",
    "        selfies_str (str): The SELFIES string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        List of tokens extracted from the SELFIES string.\n",
    "    \"\"\"\n",
    "    return list(sf.split_selfies(selfies_str))\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2. Vocabulary Building\n",
    "# ==============================\n",
    "\n",
    "def build_vocab(\n",
    "    sequences: list[str], \n",
    "    tokenizer, \n",
    "    reserved_tokens: list[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Builds a vocabulary dictionary from a list of sequences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list[str]): List of SMILES or SELFIES sequences.\n",
    "        tokenizer (callable): Function to tokenize a sequence.\n",
    "        reserved_tokens (list[str], optional): Special tokens to prepend. \n",
    "            Defaults to [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"].\n",
    "\n",
    "    Returns:\n",
    "        dict: Vocabulary containing:\n",
    "            - \"tokens\": list of all tokens\n",
    "            - \"token_to_id\": mapping token -> integer ID\n",
    "            - \"id_to_token\": mapping integer ID -> token\n",
    "    \"\"\"\n",
    "    reserved_tokens = reserved_tokens or [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "    counter = Counter()\n",
    "\n",
    "    # Count occurrences of each token across all sequences\n",
    "    for seq in sequences:\n",
    "        counter.update(tokenizer(seq))\n",
    "\n",
    "    # Sort tokens by frequency (descending), then alphabetically\n",
    "    sorted_tokens = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Combine reserved tokens with the sorted token list\n",
    "    tokens = reserved_tokens + [tok for tok, _ in sorted_tokens]\n",
    "\n",
    "    # Build bidirectional mappings\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_to_id\": {t: i for i, t in enumerate(tokens)},\n",
    "        \"id_to_token\": {i: t for i, t in enumerate(tokens)},\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3. Save Vocabulary\n",
    "# ==============================\n",
    "\n",
    "def save_vocab(vocab: dict, out_path: str):\n",
    "    \"\"\"\n",
    "    Saves a vocabulary dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict): Vocabulary dictionary (from build_vocab).\n",
    "        out_path (str): Path to save the JSON file.\n",
    "    \"\"\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(vocab, f, indent=2)\n",
    "    print(f\"Saved {len(vocab['tokens'])} tokens -> {out_path}\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4. Load Vocabulary\n",
    "# ==============================\n",
    "\n",
    "def load_vocab(vocab_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads a vocabulary dictionary from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (str): Path to a saved vocabulary JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Loaded vocabulary dictionary.\n",
    "    \"\"\"\n",
    "    with open(vocab_file, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db50460f-6bbd-4a88-85fe-a94ecc8dd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_files = [\"mol3d_data/mol3d_mil1.h5\",\n",
    "                  \"mol3d_data/mol3d_mil2.h5\",\n",
    "                  \"mol3d_data/mol3d_mil3.h5\",\n",
    "                  \"mol3d_data/mol3d_mil4.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8373bcb5-f830-44a3-9e7a-3acd952d27ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 166 tokens -> mol3d_data/smiles_vocab.json\n"
     ]
    }
   ],
   "source": [
    "smiles = []\n",
    "for fid in molecule_files:\n",
    "    with h5py.File(fid, \"r\") as f:\n",
    "        for s in f[\"smiles\"]:\n",
    "            smiles.append(s.decode(\"utf-8\"))\n",
    "smiles_vocab = build_vocab(smiles, tokenize_smiles)\n",
    "save_vocab(smiles_vocab, \"mol3d_data/smiles_vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac58b362-f099-4e9d-8f65-bb5324869e88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab/selfies_vocab.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m             selfies\u001b[38;5;241m.\u001b[39mappend(s\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m selfies_vocab \u001b[38;5;241m=\u001b[39m build_vocab(selfies, tokenize_selfies)\n\u001b[1;32m----> 7\u001b[0m save_vocab(selfies_vocab, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab/selfies_vocab.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m, in \u001b[0;36msave_vocab\u001b[1;34m(vocab, out_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_vocab\u001b[39m(vocab, out_path):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     36\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(vocab, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab/selfies_vocab.json'"
     ]
    }
   ],
   "source": [
    "selfies = []\n",
    "for fid in molecule_files:\n",
    "    with h5py.File(fid, \"r\") as f:\n",
    "        for s in f[\"selfies\"]:\n",
    "            selfies.append(s.decode(\"utf-8\"))\n",
    "selfies_vocab = build_vocab(selfies, tokenize_selfies)\n",
    "save_vocab(selfies_vocab, \"mol3d_data/selfies_vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee969eea-5dad-444d-8694-8c31a9d3ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '<bos>', '<eos>', 'C', '=', '(', ')', '1', 'N', 'O', '2', '[C@@H]', '[C@H]', 'F', 'S', 'Cl', '3', '[C]', '[N]', '[NH]', '[C@]', '[C@@]', '[N+]', '[O-]', '[O]', '[CH]', '[H]', 'Br', '[Si]', 'P', '[SH]', 'B', '4', '[S@H]', '[CH2]', '[S@@H]', '[P@]', '[S]', '[PH]', '[B]', '[P@@]', '[P]', '[Se]', '5', '[SiH]', '[SiH2]', '[Si@]', '[Si@@]', '[Ge]', '[As]', '[SiH3]', '[Si@@H]', '6', '[S@]', '[S@@]', '[Si@H]', '[P@H]', '[SeH]', '[S@OH18]', '[P@@H]', '[BH]', '[P@TB2]', '[S@TB16]', '[GeH3]', '[P@TB19]', '[S@TB17]', '[P@TB17]', '[PH2]', '[P@TB4]', '[S@TB20]', '[P@TB13]', '[P@TB14]', '[GeH2]', '[GeH]', '[S@SP3H]', '[SH2]', '[P@TB6]', '[S@TB19]', '[S@TB6]', '[P@TB9]', '7', '[P@TB10]', '[S@TB4]', '[P@TB7]', '[P@TB18]', '[S@TB5]', '[Al]', '[S@TB18]', '[S@OH23]', '[S@SP2]', '[AsH]', '[S@TB3]', '[P@TB12]', '[S@OH6]', '[S@TB2]', '[AsH2]', '[P@TB15]', '[P@TB20]', '[PH3]', '[S@OH26]', '[OH]', '[P@TB3]', '[S@TB1]', '[P@TB1]', '[P@TB8]', '[S@OH21]', '[Ti]', '[NH2]', '[P@TB16]', '[S@OH16]', '[Ge@@]', '[P@TB5]', '[PH4]', '[S@OH15]', '[S@OH20]', '[AlH]', '[Ga]', '[S@OH24]', '[S@OH3]', '[S@OH5]', '[S@SP3]', '[S@TB15]', '[SH3]', '8', '[BH2]', '[Li]', '[P@TB11]', '[S@SP1]', '[GeH4]', '[Mg]', '[Na]', '[S@OH12]', '[S@OH22]', '[S@OH28]', '[SiH4]', '[Zn]', '[BeH]', '[Be]', '[Cu]', '[Ge@]', '[P@SP2]', '[P@TB5H]', '[S@OH29]', '[S@SP1H]', '[SeH2]', '[V]', '[AlH2]', '[AlH3]', '[AsH3]', '[CH3]', '[Ca]', '[Fe]', '[GaH2]', '[GaH]', '[Ge@@H]', '[Ge@TB3]', '[Ni]', '[P@@H2]', '[P@TB6H]', '[S@OH11]', '[S@OH17]', '[S@OH27]', '[S@OH7]', '[S@SP2H]', '[Sc]'] \n",
      "\n",
      "{'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, 'C': 4, '=': 5, '(': 6, ')': 7, '1': 8, 'N': 9, 'O': 10, '2': 11, '[C@@H]': 12, '[C@H]': 13, 'F': 14, 'S': 15, 'Cl': 16, '3': 17, '[C]': 18, '[N]': 19, '[NH]': 20, '[C@]': 21, '[C@@]': 22, '[N+]': 23, '[O-]': 24, '[O]': 25, '[CH]': 26, '[H]': 27, 'Br': 28, '[Si]': 29, 'P': 30, '[SH]': 31, 'B': 32, '4': 33, '[S@H]': 34, '[CH2]': 35, '[S@@H]': 36, '[P@]': 37, '[S]': 38, '[PH]': 39, '[B]': 40, '[P@@]': 41, '[P]': 42, '[Se]': 43, '5': 44, '[SiH]': 45, '[SiH2]': 46, '[Si@]': 47, '[Si@@]': 48, '[Ge]': 49, '[As]': 50, '[SiH3]': 51, '[Si@@H]': 52, '6': 53, '[S@]': 54, '[S@@]': 55, '[Si@H]': 56, '[P@H]': 57, '[SeH]': 58, '[S@OH18]': 59, '[P@@H]': 60, '[BH]': 61, '[P@TB2]': 62, '[S@TB16]': 63, '[GeH3]': 64, '[P@TB19]': 65, '[S@TB17]': 66, '[P@TB17]': 67, '[PH2]': 68, '[P@TB4]': 69, '[S@TB20]': 70, '[P@TB13]': 71, '[P@TB14]': 72, '[GeH2]': 73, '[GeH]': 74, '[S@SP3H]': 75, '[SH2]': 76, '[P@TB6]': 77, '[S@TB19]': 78, '[S@TB6]': 79, '[P@TB9]': 80, '7': 81, '[P@TB10]': 82, '[S@TB4]': 83, '[P@TB7]': 84, '[P@TB18]': 85, '[S@TB5]': 86, '[Al]': 87, '[S@TB18]': 88, '[S@OH23]': 89, '[S@SP2]': 90, '[AsH]': 91, '[S@TB3]': 92, '[P@TB12]': 93, '[S@OH6]': 94, '[S@TB2]': 95, '[AsH2]': 96, '[P@TB15]': 97, '[P@TB20]': 98, '[PH3]': 99, '[S@OH26]': 100, '[OH]': 101, '[P@TB3]': 102, '[S@TB1]': 103, '[P@TB1]': 104, '[P@TB8]': 105, '[S@OH21]': 106, '[Ti]': 107, '[NH2]': 108, '[P@TB16]': 109, '[S@OH16]': 110, '[Ge@@]': 111, '[P@TB5]': 112, '[PH4]': 113, '[S@OH15]': 114, '[S@OH20]': 115, '[AlH]': 116, '[Ga]': 117, '[S@OH24]': 118, '[S@OH3]': 119, '[S@OH5]': 120, '[S@SP3]': 121, '[S@TB15]': 122, '[SH3]': 123, '8': 124, '[BH2]': 125, '[Li]': 126, '[P@TB11]': 127, '[S@SP1]': 128, '[GeH4]': 129, '[Mg]': 130, '[Na]': 131, '[S@OH12]': 132, '[S@OH22]': 133, '[S@OH28]': 134, '[SiH4]': 135, '[Zn]': 136, '[BeH]': 137, '[Be]': 138, '[Cu]': 139, '[Ge@]': 140, '[P@SP2]': 141, '[P@TB5H]': 142, '[S@OH29]': 143, '[S@SP1H]': 144, '[SeH2]': 145, '[V]': 146, '[AlH2]': 147, '[AlH3]': 148, '[AsH3]': 149, '[CH3]': 150, '[Ca]': 151, '[Fe]': 152, '[GaH2]': 153, '[GaH]': 154, '[Ge@@H]': 155, '[Ge@TB3]': 156, '[Ni]': 157, '[P@@H2]': 158, '[P@TB6H]': 159, '[S@OH11]': 160, '[S@OH17]': 161, '[S@OH27]': 162, '[S@OH7]': 163, '[S@SP2H]': 164, '[Sc]': 165} \n",
      "\n",
      "{'0': '<pad>', '1': '<unk>', '2': '<bos>', '3': '<eos>', '4': 'C', '5': '=', '6': '(', '7': ')', '8': '1', '9': 'N', '10': 'O', '11': '2', '12': '[C@@H]', '13': '[C@H]', '14': 'F', '15': 'S', '16': 'Cl', '17': '3', '18': '[C]', '19': '[N]', '20': '[NH]', '21': '[C@]', '22': '[C@@]', '23': '[N+]', '24': '[O-]', '25': '[O]', '26': '[CH]', '27': '[H]', '28': 'Br', '29': '[Si]', '30': 'P', '31': '[SH]', '32': 'B', '33': '4', '34': '[S@H]', '35': '[CH2]', '36': '[S@@H]', '37': '[P@]', '38': '[S]', '39': '[PH]', '40': '[B]', '41': '[P@@]', '42': '[P]', '43': '[Se]', '44': '5', '45': '[SiH]', '46': '[SiH2]', '47': '[Si@]', '48': '[Si@@]', '49': '[Ge]', '50': '[As]', '51': '[SiH3]', '52': '[Si@@H]', '53': '6', '54': '[S@]', '55': '[S@@]', '56': '[Si@H]', '57': '[P@H]', '58': '[SeH]', '59': '[S@OH18]', '60': '[P@@H]', '61': '[BH]', '62': '[P@TB2]', '63': '[S@TB16]', '64': '[GeH3]', '65': '[P@TB19]', '66': '[S@TB17]', '67': '[P@TB17]', '68': '[PH2]', '69': '[P@TB4]', '70': '[S@TB20]', '71': '[P@TB13]', '72': '[P@TB14]', '73': '[GeH2]', '74': '[GeH]', '75': '[S@SP3H]', '76': '[SH2]', '77': '[P@TB6]', '78': '[S@TB19]', '79': '[S@TB6]', '80': '[P@TB9]', '81': '7', '82': '[P@TB10]', '83': '[S@TB4]', '84': '[P@TB7]', '85': '[P@TB18]', '86': '[S@TB5]', '87': '[Al]', '88': '[S@TB18]', '89': '[S@OH23]', '90': '[S@SP2]', '91': '[AsH]', '92': '[S@TB3]', '93': '[P@TB12]', '94': '[S@OH6]', '95': '[S@TB2]', '96': '[AsH2]', '97': '[P@TB15]', '98': '[P@TB20]', '99': '[PH3]', '100': '[S@OH26]', '101': '[OH]', '102': '[P@TB3]', '103': '[S@TB1]', '104': '[P@TB1]', '105': '[P@TB8]', '106': '[S@OH21]', '107': '[Ti]', '108': '[NH2]', '109': '[P@TB16]', '110': '[S@OH16]', '111': '[Ge@@]', '112': '[P@TB5]', '113': '[PH4]', '114': '[S@OH15]', '115': '[S@OH20]', '116': '[AlH]', '117': '[Ga]', '118': '[S@OH24]', '119': '[S@OH3]', '120': '[S@OH5]', '121': '[S@SP3]', '122': '[S@TB15]', '123': '[SH3]', '124': '8', '125': '[BH2]', '126': '[Li]', '127': '[P@TB11]', '128': '[S@SP1]', '129': '[GeH4]', '130': '[Mg]', '131': '[Na]', '132': '[S@OH12]', '133': '[S@OH22]', '134': '[S@OH28]', '135': '[SiH4]', '136': '[Zn]', '137': '[BeH]', '138': '[Be]', '139': '[Cu]', '140': '[Ge@]', '141': '[P@SP2]', '142': '[P@TB5H]', '143': '[S@OH29]', '144': '[S@SP1H]', '145': '[SeH2]', '146': '[V]', '147': '[AlH2]', '148': '[AlH3]', '149': '[AsH3]', '150': '[CH3]', '151': '[Ca]', '152': '[Fe]', '153': '[GaH2]', '154': '[GaH]', '155': '[Ge@@H]', '156': '[Ge@TB3]', '157': '[Ni]', '158': '[P@@H2]', '159': '[P@TB6H]', '160': '[S@OH11]', '161': '[S@OH17]', '162': '[S@OH27]', '163': '[S@OH7]', '164': '[S@SP2H]', '165': '[Sc]'}\n"
     ]
    }
   ],
   "source": [
    "vocab = load_vocab(\"mol3d_data/smiles_vocab.json\")\n",
    "print(vocab[\"tokens\"], '\\n')\n",
    "print(vocab[\"token_to_id\"], '\\n')\n",
    "print(vocab[\"id_to_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbbe89-677d-45dd-b020-bcc2c18361ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
