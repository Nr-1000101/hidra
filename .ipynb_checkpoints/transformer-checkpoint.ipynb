{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5878bc1f-7f59-43cc-8e96-cf40d0d6f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Symmetry Transformer: Hierarchical Transformer for predicting molecular geometry \n",
    "from SMILES or SELFIES strings, loading data from HDF5 (.h5) files.\n",
    "\n",
    "Architecture:\n",
    "    1. Shared initial encoder blocks process input sequence\n",
    "    2. Hierarchical property predictions:\n",
    "       - Each property cross-attends to previous predictions\n",
    "       - Processes with its own encoder blocks\n",
    "       - Makes prediction and embeds it for next property\n",
    "    3. Properties flow: dimension → rings → chirality → symmetry → point_group → planarity → angles\n",
    "\"\"\"\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import selfies as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Vocab Loader\n",
    "# -----------------------------\n",
    "def load_vocab(vocab_file: str) -> Tuple[List[str], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Load vocabulary from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        vocab_file: Path to vocab JSON file\n",
    "        \n",
    "    Returns:\n",
    "        tokens: List of all tokens\n",
    "        token_to_id: Token string to ID mapping\n",
    "        id_to_token: ID to token string mapping\n",
    "    \"\"\"\n",
    "    with open(vocab_file, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return vocab[\"tokens\"], vocab[\"token_to_id\"], {int(k): v for k, v in vocab[\"id_to_token\"].items()}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Label Encoder\n",
    "# -----------------------------\n",
    "class LabelEncoder:\n",
    "    \"\"\"Encodes string labels to integer indices for classification tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_to_idx: Dict[str, Dict[str, int]] = {}\n",
    "        self.idx_to_label: Dict[str, Dict[int, str]] = {}\n",
    "    \n",
    "    def fit(self, property_name: str, labels: List[str]):\n",
    "        \"\"\"Create encoding for a property's labels.\n",
    "        \n",
    "        Args:\n",
    "            property_name: Name of the property (e.g., 'dimension')\n",
    "            labels: List of unique string labels\n",
    "        \"\"\"\n",
    "        unique_labels = sorted(set(labels))\n",
    "        self.label_to_idx[property_name] = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.idx_to_label[property_name] = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    def transform(self, property_name: str, label: str) -> int:\n",
    "        \"\"\"Convert label to index.\"\"\"\n",
    "        return self.label_to_idx[property_name].get(label, 0)  # Default to 0 if unknown\n",
    "    \n",
    "    def inverse_transform(self, property_name: str, idx: int) -> str:\n",
    "        \"\"\"Convert index back to label.\"\"\"\n",
    "        return self.idx_to_label[property_name].get(idx, \"unknown\")\n",
    "    \n",
    "    def get_num_classes(self, property_name: str) -> int:\n",
    "        \"\"\"Get number of classes for a property.\"\"\"\n",
    "        return len(self.label_to_idx.get(property_name, {}))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SELFIES Tokenizer\n",
    "# -----------------------------\n",
    "class SelfiesTokenizer:\n",
    "    \"\"\"Tokenizer for SELFIES molecular representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_file: str):\n",
    "        \"\"\"Initialize tokenizer from vocabulary file.\n",
    "        \n",
    "        Args:\n",
    "            vocab_file: Path to SELFIES vocabulary JSON\n",
    "        \"\"\"\n",
    "        self.tokens, self.token_to_id, self.id_to_token = load_vocab(vocab_file)\n",
    "        self.reserved_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "    def encode(self, s: str, add_special: bool = True) -> List[int]:\n",
    "        \"\"\"Encode SELFIES string to token IDs.\n",
    "        \n",
    "        Args:\n",
    "            s: SELFIES string\n",
    "            add_special: Whether to add BOS/EOS tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        toks = list(sf.split_selfies(s))\n",
    "        ids = [self.token_to_id.get(t, self.token_to_id[\"<unk>\"]) for t in toks]\n",
    "        if add_special:\n",
    "            return [self.token_to_id[\"<bos>\"]] + ids + [self.token_to_id[\"<eos>\"]]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to SELFIES string.\n",
    "        \n",
    "        Args:\n",
    "            ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            SELFIES string\n",
    "        \"\"\"\n",
    "        toks = [self.id_to_token.get(i, \"?\") for i in ids]\n",
    "        toks = [t for t in toks if t not in self.reserved_tokens]\n",
    "        return sf.decoder(\"\".join(toks))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SMILES Tokenizer\n",
    "# -----------------------------\n",
    "SMILES_REGEX = re.compile(\n",
    "    r\"\"\"([A-Z][a-z]?               # single and multi-letter atoms\n",
    "        | \\[ [^\\]]+ \\]             # bracketed expressions\n",
    "        | @@?                      # stereochemistry @ / @@\n",
    "        | =|#|\\+|-|/|\\\\            # bonds\n",
    "        | \\(|\\)|\\.|\\*              # parentheses, dot, wildcard\n",
    "        | \\d                       # single-digit ring closures\n",
    "        | %\\d{2}                   # two-digit ring closures\n",
    "    )\"\"\",\n",
    "    re.X\n",
    ")\n",
    "\n",
    "class SmilesTokenizer:\n",
    "    \"\"\"Tokenizer for SMILES molecular representations.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_file: str):\n",
    "        \"\"\"Initialize tokenizer from vocabulary file.\n",
    "        \n",
    "        Args:\n",
    "            vocab_file: Path to SMILES vocabulary JSON\n",
    "        \"\"\"\n",
    "        self.tokens, self.token_to_id, self.id_to_token = load_vocab(vocab_file)\n",
    "        self.reserved_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "    def tokenize(self, smiles: str) -> List[str]:\n",
    "        \"\"\"Tokenize SMILES string into tokens.\"\"\"\n",
    "        return [t for t in SMILES_REGEX.findall(smiles) if t]\n",
    "\n",
    "    def encode(self, s: str, add_special: bool = True) -> List[int]:\n",
    "        \"\"\"Encode SMILES string to token IDs.\n",
    "        \n",
    "        Args:\n",
    "            s: SMILES string\n",
    "            add_special: Whether to add BOS/EOS tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        toks = self.tokenize(s)\n",
    "        ids = [self.token_to_id.get(t, self.token_to_id[\"<unk>\"]) for t in toks]\n",
    "        if add_special:\n",
    "            return [self.token_to_id[\"<bos>\"]] + ids + [self.token_to_id[\"<eos>\"]]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to SMILES string.\n",
    "        \n",
    "        Args:\n",
    "            ids: List of token IDs\n",
    "            \n",
    "        Returns:\n",
    "            SMILES string\n",
    "        \"\"\"\n",
    "        toks = [self.id_to_token.get(i, \"?\") for i in ids]\n",
    "        toks = [t for t in toks if t not in self.reserved_tokens]\n",
    "        return \"\".join(toks)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset from HDF5\n",
    "# -----------------------------\n",
    "class H5SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for loading molecular sequences and features from HDF5 files.\n",
    "    \n",
    "    Supports multiple HDF5 files and handles:\n",
    "    - SMILES/SELFIES tokenization\n",
    "    - Feature extraction (dimensions, symmetry, rings, etc.)\n",
    "    - Label binning for underrepresented classes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mol_files: List[str],\n",
    "        feat_files: List[str],\n",
    "        tokenizer,\n",
    "        label_encoder: LabelEncoder,\n",
    "        underrepresented_data_file: str = \"mol3d_data/underrepresented_data.json\",\n",
    "        mode: str = \"smiles\",\n",
    "        max_len: int = 512\n",
    "    ):\n",
    "        \"\"\"Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            mol_files: List of HDF5 files with molecule sequences\n",
    "            feat_files: List of HDF5 files with molecular features\n",
    "            tokenizer: Tokenizer instance (SMILES or SELFIES)\n",
    "            label_encoder: LabelEncoder for string→int conversion\n",
    "            underrepresented_data_file: JSON with binning thresholds\n",
    "            mode: \"smiles\" or \"selfies\"\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.mol_files = mol_files\n",
    "        self.feat_files = feat_files\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "\n",
    "        # Precompute file offsets for efficient indexing\n",
    "        self.file_offsets = [0]\n",
    "        for mf in mol_files:\n",
    "            with h5py.File(mf, \"r\") as f:\n",
    "                n_mols = len(f[mode])\n",
    "                self.file_offsets.append(self.file_offsets[-1] + n_mols)\n",
    "\n",
    "        # Precompute index map: (file_id, index_in_file)\n",
    "        self.entries = []\n",
    "        for fid, mf in enumerate(mol_files):\n",
    "            with h5py.File(mf, \"r\") as f:\n",
    "                n_mols = len(f[mode])\n",
    "                for idx in range(n_mols):\n",
    "                    self.entries.append((fid, idx))\n",
    "\n",
    "        # Load ring info into memory for fast access\n",
    "        self.load_ring_info()\n",
    "        \n",
    "        # Load binning thresholds\n",
    "        with open(underrepresented_data_file, \"r\") as f:\n",
    "            self.underrepresented_groups = json.load(f)\n",
    "\n",
    "    def load_ring_info(self):\n",
    "        \"\"\"Load ring-related datasets into memory for fast access.\"\"\"\n",
    "        ring_counts = []\n",
    "        planar_fit_errors = []\n",
    "        ring_plane_angles = []\n",
    "\n",
    "        for ff in self.feat_files:\n",
    "            with h5py.File(ff, \"r\") as f:\n",
    "                rc = f[\"nrings\"][:]\n",
    "                ring_counts.extend(rc)\n",
    "                planar_fit_errors.extend(f[\"errors\"][:])\n",
    "\n",
    "                # Plane angles stored as structured array\n",
    "                trip_arr = f[\"plane_angles\"][:]\n",
    "                triplets = [(int(i), int(j), float(val)) \n",
    "                            for i, j, val in zip(trip_arr[\"i\"], trip_arr[\"j\"], trip_arr[\"val\"])]\n",
    "                \n",
    "                # Group angles by molecule\n",
    "                count = 0\n",
    "                for n_rings in rc:\n",
    "                    n_pairs = n_rings * (n_rings - 1) // 2\n",
    "                    mol_angles = triplets[count:count + n_pairs]\n",
    "                    ring_plane_angles.append(mol_angles)\n",
    "                    count += n_pairs\n",
    "        \n",
    "        self.ring_counts = ring_counts\n",
    "        self.planar_fit_errors = planar_fit_errors\n",
    "        self.ring_plane_angles = ring_plane_angles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def assign_binned_labels(self, label: int, threshold: int) -> str:\n",
    "        \"\"\"Bin labels with catch-all for values >= threshold.\n",
    "        \n",
    "        Args:\n",
    "            label: Integer label value\n",
    "            threshold: Threshold for catch-all bin\n",
    "            \n",
    "        Returns:\n",
    "            String label (e.g., \"0\", \"1\", \"5+\")\n",
    "        \"\"\"\n",
    "        return str(label) if label < threshold else f\"{threshold}+\"\n",
    "\n",
    "    def assign_other_label(self, label: str, labels_to_change: List[str], new_label: str = \"Other\") -> str:\n",
    "        \"\"\"Replace underrepresented labels with catch-all.\n",
    "        \n",
    "        Args:\n",
    "            label: Original label\n",
    "            labels_to_change: Labels to replace\n",
    "            new_label: Replacement label\n",
    "            \n",
    "        Returns:\n",
    "            Modified label\n",
    "        \"\"\"\n",
    "        return new_label if label in labels_to_change else label\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Get a single data sample.\n",
    "        \n",
    "        Args:\n",
    "            i: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            input_ids: Tokenized sequence (padded)\n",
    "            attention_mask: Mask for valid tokens\n",
    "            targets: Dictionary of target values for each property\n",
    "        \"\"\"\n",
    "        fid, idx = self.entries[i]\n",
    "\n",
    "        # Load and tokenize sequence\n",
    "        with h5py.File(self.mol_files[fid], \"r\") as f_mol:\n",
    "            seq = f_mol[self.mode][idx].decode(\"utf-8\")\n",
    "\n",
    "        # Tokenize with BOS/EOS and pad/truncate\n",
    "        pad_id = self.tokenizer.token_to_id[\"<pad>\"]\n",
    "        bos_id = self.tokenizer.token_to_id[\"<bos>\"]\n",
    "        eos_id = self.tokenizer.token_to_id[\"<eos>\"]\n",
    "        \n",
    "        tokens = [bos_id] + self.tokenizer.encode(seq, add_special=False)\n",
    "        if len(tokens) >= self.max_len:\n",
    "            tokens = tokens[:self.max_len - 1]\n",
    "        tokens = tokens + [eos_id]\n",
    "        \n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens = tokens + [pad_id] * (self.max_len - len(tokens))\n",
    "        \n",
    "        input_ids = torch.tensor(tokens, dtype=torch.long)\n",
    "        attention_mask = (input_ids != pad_id).long()\n",
    "\n",
    "        # Load features\n",
    "        with h5py.File(self.feat_files[fid], \"r\") as f_feat:\n",
    "            dimension = f_feat[\"dimensions\"][idx].decode(\"utf-8\")\n",
    "            point_group = self.assign_other_label(\n",
    "                f_feat[\"point_groups\"][idx].decode(\"utf-8\"),\n",
    "                self.underrepresented_groups[\"point_groups\"]\n",
    "            )\n",
    "            n_symmetry_planes = self.assign_binned_labels(\n",
    "                int(f_feat[\"symmetry_planes\"][idx]),\n",
    "                self.underrepresented_groups[\"symmetry_planes\"]\n",
    "            )\n",
    "            chirality = bool(f_feat[\"chiralities\"][idx])\n",
    "\n",
    "        # Get preloaded ring info using global index\n",
    "        global_idx = self.file_offsets[fid] + idx\n",
    "        ring_count = self.assign_binned_labels(\n",
    "            self.ring_counts[global_idx],\n",
    "            self.underrepresented_groups[\"nrings\"]\n",
    "        )\n",
    "        planar_fit_error = self.planar_fit_errors[global_idx]\n",
    "        ring_plane_angles = self.ring_plane_angles[global_idx]\n",
    "\n",
    "        # Convert to tensor targets\n",
    "        targets = {\n",
    "            \"dimension\": self.label_encoder.transform(\"dimension\", dimension),\n",
    "            \"ring_count\": self.label_encoder.transform(\"ring_count\", ring_count),\n",
    "            \"chirality\": int(chirality),\n",
    "            \"n_symmetry_planes\": self.label_encoder.transform(\"n_symmetry_planes\", n_symmetry_planes),\n",
    "            \"point_group\": self.label_encoder.transform(\"point_group\", point_group),\n",
    "            \"planar_fit_error\": planar_fit_error,\n",
    "            \"ring_plane_angles\": len(ring_plane_angles),  # Simplified: just count for now\n",
    "        }\n",
    "\n",
    "        return input_ids, attention_mask, targets\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (input_ids, attention_mask, targets) tuples\n",
    "        \n",
    "    Returns:\n",
    "        Batched tensors\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    attention_masks = torch.stack([item[1] for item in batch])\n",
    "    \n",
    "    # Stack targets for each property\n",
    "    target_keys = batch[0][2].keys()\n",
    "    targets = {\n",
    "        key: torch.tensor([item[2][key] for item in batch])\n",
    "        for key in target_keys\n",
    "    }\n",
    "    \n",
    "    return input_ids, attention_masks, targets\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Positional Encoding\n",
    "# -----------------------------\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learned positional embeddings for sequence modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 512, max_len: int = 512):\n",
    "        \"\"\"Initialize positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Embedding dimension\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        nn.init.normal_(self.pe.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional embeddings to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            x + positional embeddings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pe(positions)\n",
    "        return x + pos_emb\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Cross-Attention Module\n",
    "# -----------------------------\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention layer for conditioning on previous property predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, dropout: float = 0.1):\n",
    "        \"\"\"Initialize cross-attention.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            nhead: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        memory_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply cross-attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Query tensor (current property)\n",
    "            memory: Key/Value tensor (previous property embeddings)\n",
    "            memory_mask: Padding mask for memory\n",
    "            \n",
    "        Returns:\n",
    "            Updated x with cross-attention applied\n",
    "        \"\"\"\n",
    "        attn_out, _ = self.attn(x, memory, memory, key_padding_mask=memory_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Property Block\n",
    "# -----------------------------\n",
    "class PropertyBlock(nn.Module):\n",
    "    \"\"\"Property prediction block with encoder layers, cross-attention, and prediction head.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Cross-attention to previous property (if available)\n",
    "        2. N transformer encoder layers\n",
    "        3. Prediction head (classification or regression)\n",
    "        4. Prediction embedding for next property\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        task: str = \"classification\",\n",
    "        num_classes: Optional[int] = None\n",
    "    ):\n",
    "        \"\"\"Initialize property block.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            nhead: Number of attention heads\n",
    "            num_layers: Number of encoder layers in this block\n",
    "            dim_feedforward: FFN dimension\n",
    "            dropout: Dropout probability\n",
    "            task: \"classification\" or \"regression\"\n",
    "            num_classes: Number of classes (for classification)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Cross-attention for previous property conditioning\n",
    "        self.cross_attn = CrossAttention(d_model, nhead, dropout)\n",
    "        \n",
    "        # Encoder layers for processing after cross-attention\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Prediction head\n",
    "        if task == \"classification\":\n",
    "            assert num_classes is not None, \"num_classes required for classification\"\n",
    "            self.head = nn.Linear(d_model, num_classes)\n",
    "        elif task == \"regression\":\n",
    "            self.head = nn.Linear(d_model, 1)\n",
    "        else:\n",
    "            raise ValueError(\"task must be 'classification' or 'regression'\")\n",
    "        \n",
    "        # Embed prediction for next property\n",
    "        if task == \"classification\":\n",
    "            self.pred_embedding = nn.Embedding(num_classes, d_model)\n",
    "        else:\n",
    "            self.pred_embedding = nn.Linear(1, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: torch.Tensor,\n",
    "        prev_memory: Optional[torch.Tensor] = None,\n",
    "        prev_memory_mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len, d_model)\n",
    "            attn_mask: Attention mask (batch_size, seq_len)\n",
    "            prev_memory: Previous property embeddings\n",
    "            prev_memory_mask: Mask for previous property\n",
    "            \n",
    "        Returns:\n",
    "            logits: Prediction logits\n",
    "            pred_embedding: Embedded prediction for next property\n",
    "        \"\"\"\n",
    "        # Apply cross-attention if previous property exists\n",
    "        if prev_memory is not None:\n",
    "            x = self.cross_attn(x, prev_memory, memory_mask=~prev_memory_mask.bool())\n",
    "        \n",
    "        # Process with encoder layers\n",
    "        h = self.encoder(x, src_key_padding_mask=~attn_mask.bool())\n",
    "        \n",
    "        # Pool for prediction (use CLS token = first token)\n",
    "        pooled = h[:, 0]  # (batch_size, d_model)\n",
    "        logits = self.head(pooled)\n",
    "        \n",
    "        # Embed prediction for next property\n",
    "        if self.task == \"classification\":\n",
    "            pred_idx = logits.argmax(dim=-1)  # (batch_size,)\n",
    "            pred_emb = self.pred_embedding(pred_idx)  # (batch_size, d_model)\n",
    "        else:\n",
    "            pred_emb = self.pred_embedding(logits)  # (batch_size, d_model)\n",
    "        \n",
    "        # Expand to sequence length for cross-attention\n",
    "        pred_emb = pred_emb.unsqueeze(1).expand(-1, h.size(1), -1)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return logits, pred_emb\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Hierarchical Transformer\n",
    "# -----------------------------\n",
    "class HierarchicalTransformer(nn.Module):\n",
    "    \"\"\"Hierarchical Transformer for molecular property prediction.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Token + positional embeddings\n",
    "        2. N initial shared encoder blocks\n",
    "        3. Hierarchical property predictions:\n",
    "           - Each property cross-attends to previous predictions\n",
    "           - Processes with its own encoder blocks\n",
    "           - Makes prediction and embeds it for next property\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        property_configs: List[Dict],\n",
    "        max_len: int = 512,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        n_initial_blocks: int = 4,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        pad_idx: int = 0\n",
    "    ):\n",
    "        \"\"\"Initialize hierarchical transformer.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of token vocabulary\n",
    "            property_configs: List of dicts with property configurations:\n",
    "                [\n",
    "                    {\"task\": \"classification\", \"num_classes\": 5, \"n_blocks\": 2},\n",
    "                    {\"task\": \"regression\", \"n_blocks\": 1},\n",
    "                    ...\n",
    "                ]\n",
    "            max_len: Maximum sequence length\n",
    "            d_model: Model dimension\n",
    "            nhead: Number of attention heads\n",
    "            n_initial_blocks: Number of shared encoder blocks before properties\n",
    "            dim_feedforward: FFN dimension\n",
    "            dropout: Dropout probability\n",
    "            pad_idx: Padding token index\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_enc = LearnedPositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Initial shared encoder\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward, dropout, batch_first=True\n",
    "        )\n",
    "        self.initial_encoder = nn.TransformerEncoder(enc_layer, num_layers=n_initial_blocks)\n",
    "        \n",
    "        # Property blocks\n",
    "        self.properties = nn.ModuleList([\n",
    "            PropertyBlock(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                num_layers=cfg.get(\"n_blocks\", 2),\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                task=cfg[\"task\"],\n",
    "                num_classes=cfg.get(\"num_classes\")\n",
    "            )\n",
    "            for cfg in property_configs\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Forward pass through hierarchical network.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of predictions for each property\n",
    "        \"\"\"\n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.token_emb(input_ids)\n",
    "        x = self.pos_enc(x)\n",
    "        \n",
    "        # Initial shared encoding\n",
    "        x = self.initial_encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
    "        \n",
    "        # Hierarchical property predictions\n",
    "        outputs = {}\n",
    "        prev_memory = None\n",
    "        prev_mask = None\n",
    "        \n",
    "        for i, block in enumerate(self.properties):\n",
    "            logits, pred_emb = block(\n",
    "                x,\n",
    "                attention_mask,\n",
    "                prev_memory=prev_memory,\n",
    "                prev_memory_mask=prev_mask\n",
    "            )\n",
    "            outputs[f\"prop_{i}\"] = logits\n",
    "            \n",
    "            # Feed prediction embedding to next property\n",
    "            prev_memory = pred_emb\n",
    "            prev_mask = attention_mask\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training & Evaluation\n",
    "# -----------------------------\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    property_info: List[Tuple[str, int]],\n",
    "    bf16: bool = True\n",
    ") -> float:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loss_fns = []\n",
    "    for task, num_classes in property_info:\n",
    "        if task == \"classification\":\n",
    "            loss_fns.append(nn.CrossEntropyLoss())\n",
    "        else:\n",
    "            loss_fns.append(nn.MSELoss())\n",
    "\n",
    "    for input_ids, attention_mask, targets in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = {k: v.to(device) for k, v in targets.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        autocast_ctx = (\n",
    "            torch.autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "            if bf16 else torch.cuda.amp.autocast(enabled=False)\n",
    "        )\n",
    "        \n",
    "        with autocast_ctx:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = 0.0\n",
    "            \n",
    "            # Match targets to outputs by explicit ordering\n",
    "            target_keys = [\"dimension\", \"ring_count\", \"chirality\", \n",
    "                          \"n_symmetry_planes\", \"point_group\", \n",
    "                          \"planar_fit_error\", \"ring_plane_angles\"]\n",
    "            \n",
    "            for i, (out_key, fn) in enumerate(zip(outputs.keys(), loss_fns)):\n",
    "                target = targets[target_keys[i]]\n",
    "                pred = outputs[out_key]\n",
    "                \n",
    "                if property_info[i][0] == \"classification\":\n",
    "                    loss += fn(pred, target.long())\n",
    "                else:\n",
    "                    loss += fn(pred.squeeze(-1), target.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    property_info: List[Tuple[str, int]],\n",
    "    bf16: bool = False\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    loss_fns = []\n",
    "    for task, num_classes in property_info:\n",
    "        if task == \"classification\":\n",
    "            loss_fns.append(nn.CrossEntropyLoss())\n",
    "        else:\n",
    "            loss_fns.append(nn.MSELoss())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        autocast_ctx = (\n",
    "            torch.autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "            if bf16 else torch.cuda.amp.autocast(enabled=False)\n",
    "        )\n",
    "        \n",
    "        with autocast_ctx:\n",
    "            for input_ids, attention_mask, targets in dataloader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                targets = {k: v.to(device) for k, v in targets.items()}\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = 0.0\n",
    "                \n",
    "                target_keys = [\"dimension\", \"ring_count\", \"chirality\", \n",
    "                              \"n_symmetry_planes\", \"point_group\", \n",
    "                              \"planar_fit_error\", \"ring_plane_angles\"]\n",
    "                \n",
    "                for i, (out_key, fn) in enumerate(zip(outputs.keys(), loss_fns)):\n",
    "                    target = targets[target_keys[i]]\n",
    "                    pred = outputs[out_key]\n",
    "                    \n",
    "                    if property_info[i][0] == \"classification\":\n",
    "                        loss += fn(pred, target.long())\n",
    "                    else:\n",
    "                        loss += fn(pred.squeeze(-1), target.float())\n",
    "                \n",
    "                total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "    mse = total_loss / len(dataloader.dataset)\n",
    "    rmse = mse ** 0.5\n",
    "    return mse, rmse\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main Training Script\n",
    "# -----------------------------\n",
    "def main():\n",
    "    \"\"\"Main training loop.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Train Hierarchical Transformer for molecular properties\")\n",
    "    parser.add_argument(\"--mol_files\", nargs='+', required=True, help=\"HDF5 files with molecules\")\n",
    "    parser.add_argument(\"--feat_files\", nargs='+', required=True, help=\"HDF5 files with features\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=20, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
    "    parser.add_argument(\"--max_len\", type=int, default=512, help=\"Maximum sequence length\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--d_model\", type=int, default=512, help=\"Model dimension\")\n",
    "    parser.add_argument(\"--n_initial_blocks\", type=int, default=4, help=\"Number of initial shared encoder blocks\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"smiles\", \"selfies\"], default=\"smiles\", help=\"Input format\")\n",
    "    parser.add_argument(\"--device\", default=None, help=\"Device (cuda/cpu)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    device = torch.device(\n",
    "        args.device if args.device is not None \n",
    "        else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    vocab_file = f\"mol3d_data/{args.mode}_vocab.json\"\n",
    "    if args.mode == \"smiles\":\n",
    "        tokenizer = SmilesTokenizer(vocab_file)\n",
    "    else:\n",
    "        tokenizer = SelfiesTokenizer(vocab_file)\n",
    "    print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "    # Initialize label encoder and fit on all data\n",
    "    label_encoder = LabelEncoder()\n",
    "    print(\"Fitting label encoder...\")\n",
    "    \n",
    "    # Collect all labels from all files\n",
    "    all_dimensions = []\n",
    "    all_ring_counts = []\n",
    "    all_symmetry_planes = []\n",
    "    all_point_groups = []\n",
    "    \n",
    "    with open(\"mol3d_data/underrepresented_data.json\", \"r\") as f:\n",
    "        underrep = json.load(f)\n",
    "    \n",
    "    for feat_file in args.feat_files:\n",
    "        with h5py.File(feat_file, \"r\") as f:\n",
    "            all_dimensions.extend([d.decode(\"utf-8\") for d in f[\"dimensions\"][:]])\n",
    "            \n",
    "            # Point groups with \"Other\" mapping\n",
    "            for pg in f[\"point_groups\"][:]:\n",
    "                pg_str = pg.decode(\"utf-8\")\n",
    "                if pg_str in underrep[\"point_groups\"]:\n",
    "                    all_point_groups.append(\"Other\")\n",
    "                else:\n",
    "                    all_point_groups.append(pg_str)\n",
    "            \n",
    "            # Symmetry planes with binning\n",
    "            for sp in f[\"symmetry_planes\"][:]:\n",
    "                sp_int = int(sp)\n",
    "                thresh = underrep[\"symmetry_planes\"]\n",
    "                all_symmetry_planes.append(str(sp_int) if sp_int < thresh else f\"{thresh}+\")\n",
    "            \n",
    "            # Ring counts with binning\n",
    "            for rc in f[\"nrings\"][:]:\n",
    "                thresh = underrep[\"nrings\"]\n",
    "                all_ring_counts.append(str(rc) if rc < thresh else f\"{thresh}+\")\n",
    "    \n",
    "    label_encoder.fit(\"dimension\", all_dimensions)\n",
    "    label_encoder.fit(\"ring_count\", all_ring_counts)\n",
    "    label_encoder.fit(\"n_symmetry_planes\", all_symmetry_planes)\n",
    "    label_encoder.fit(\"point_group\", all_point_groups)\n",
    "    \n",
    "    print(f\"  Dimension classes: {label_encoder.get_num_classes('dimension')}\")\n",
    "    print(f\"  Ring count classes: {label_encoder.get_num_classes('ring_count')}\")\n",
    "    print(f\"  Symmetry plane classes: {label_encoder.get_num_classes('n_symmetry_planes')}\")\n",
    "    print(f\"  Point group classes: {label_encoder.get_num_classes('point_group')}\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = H5SequenceDataset(\n",
    "        args.mol_files,\n",
    "        args.feat_files,\n",
    "        tokenizer,\n",
    "        label_encoder,\n",
    "        mode=args.mode,\n",
    "        max_len=args.max_len\n",
    "    )\n",
    "    \n",
    "    # Split dataset\n",
    "    n = len(dataset)\n",
    "    ntrain = int(0.8 * n)\n",
    "    nval = int(0.1 * n)\n",
    "    train_ds, val_ds, test_ds = random_split(dataset, [ntrain, nval, n - ntrain - nval])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Dataset: {ntrain} train, {nval} val, {n - ntrain - nval} test\")\n",
    "\n",
    "    # Define property configurations\n",
    "    property_configs = [\n",
    "        {\n",
    "            \"task\": \"classification\",\n",
    "            \"num_classes\": label_encoder.get_num_classes(\"dimension\"),\n",
    "            \"n_blocks\": 2\n",
    "        },  # dimension\n",
    "        {\n",
    "            \"task\": \"classification\",\n",
    "            \"num_classes\": label_encoder.get_num_classes(\"ring_count\"),\n",
    "            \"n_blocks\": 2\n",
    "        },  # ring_count\n",
    "        {\n",
    "            \"task\": \"classification\",\n",
    "            \"num_classes\": 2,\n",
    "            \"n_blocks\": 2\n",
    "        },  # chirality\n",
    "        {\n",
    "            \"task\": \"classification\",\n",
    "            \"num_classes\": label_encoder.get_num_classes(\"n_symmetry_planes\"),\n",
    "            \"n_blocks\": 2\n",
    "        },  # n_symmetry_planes\n",
    "        {\n",
    "            \"task\": \"classification\",\n",
    "            \"num_classes\": label_encoder.get_num_classes(\"point_group\"),\n",
    "            \"n_blocks\": 2\n",
    "        },  # point_group\n",
    "        {\n",
    "            \"task\": \"regression\",\n",
    "            \"n_blocks\": 2\n",
    "        },  # planar_fit_error\n",
    "        {\n",
    "            \"task\": \"regression\",\n",
    "            \"n_blocks\": 2\n",
    "        },  # ring_plane_angles\n",
    "    ]\n",
    "\n",
    "    # Create model\n",
    "    model = HierarchicalTransformer(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        property_configs=property_configs,\n",
    "        max_len=args.max_len,\n",
    "        d_model=args.d_model,\n",
    "        nhead=8,\n",
    "        n_initial_blocks=args.n_initial_blocks,\n",
    "        pad_idx=tokenizer.token_to_id[\"<pad>\"]\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "    # Prepare property info for training\n",
    "    property_info = [\n",
    "        (cfg[\"task\"], cfg.get(\"num_classes\", 1))\n",
    "        for cfg in property_configs\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    best_val_rmse = float(\"inf\")\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device, property_info, bf16=True)\n",
    "        val_mse, val_rmse = evaluate(model, val_loader, device, property_info, bf16=True)\n",
    "        \n",
    "        print(f\"Epoch {epoch:3d} | Train loss: {train_loss:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            torch.save({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"property_configs\": property_configs,\n",
    "                \"vocab_size\": tokenizer.vocab_size,\n",
    "                \"max_len\": args.max_len,\n",
    "                \"d_model\": args.d_model,\n",
    "                \"n_initial_blocks\": args.n_initial_blocks,\n",
    "                \"label_encoder\": label_encoder,\n",
    "            }, \"best_model.pt\")\n",
    "            print(f\"  → Saved best model (val RMSE: {val_rmse:.4f})\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_mse, test_rmse = evaluate(model, test_loader, device, property_info, bf16=True)\n",
    "    print(f\"\\nTest RMSE: {test_rmse:.4f} (MSE {test_mse:.6f})\")\n",
    "    print(f\"Best validation RMSE: {best_val_rmse:.4f}\")\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151f542-3544-44f5-bef1-46131b258201",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_files = [\"mol3d_data/mol3d_mil1.h5\",\n",
    "                  \"mol3d_data/mol3d_mil2.h5\",\n",
    "                  \"mol3d_data/mol3d_mil3.h5\",\n",
    "                  \"mol3d_data/mol3d_mil4.h5\"]\n",
    "feature_files = [\"mol3d_data/mol3d_feat_mil1.h5\",\n",
    "                \"mol3d_data/mol3d_feat_mil2.h5\",\n",
    "                \"mol3d_data/mol3d_feat_mil3.h5\",\n",
    "                \"mol3d_data/mol3d_feat_mil4.h5\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
